{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Persian_Poetry_FineTuning_12_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsiQI9Ka-HyB"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjuVHWEN-OeN"
      },
      "source": [
        "!pip install -qU transformers\n",
        "!pip install -qU hazm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOC4y5rm-QNX"
      },
      "source": [
        "import hazm\n",
        "normalizer = hazm.Normalizer(persian_numbers=False)\n",
        "\n",
        "def normalize_input(text):\n",
        "    text = normalizer.normalize(text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NNowE9KF-LM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_2U9J1o5Gtu"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data_load:\n",
        "  def __init__(self,max_length, step):\n",
        "    self.vocab = []\n",
        "    self.max_length = max_length\n",
        "    self.step = step\n",
        "            \n",
        "  def read_data(self,paths):\n",
        "    poems_sequence = []\n",
        "    poems = pd.read_csv(paths[0])\n",
        "    i = 1\n",
        "    while True:\n",
        "            poem_i = poems[poems['poem_id'] == i]\n",
        "            # print(poem_i)\n",
        "            index_i = poems.index[poems['poem_id'] == i]\n",
        "            current_poem = \"\"\n",
        "            for p in index_i:\n",
        "                v_position = poem_i.loc[p,\"v_position\"]\n",
        "                verse =  poem_i.loc[p,\"poem_text\"]\n",
        "                # print(verse)\n",
        "                current_poem += verse \n",
        "                if v_position == 0:\n",
        "                    current_poem += \" \\t\"\n",
        "                if v_position == 1:\n",
        "                    current_poem += \" \\n\"\n",
        "            if len(current_poem)>0:\n",
        "              poems_sequence.append(current_poem)\n",
        "            i += 1\n",
        "            if i>599 :\n",
        "              break\n",
        "    for path in paths[1:]:\n",
        "        \n",
        "        poems = pd.read_csv(path)\n",
        "        # print(poems['poem_id'])\n",
        "        i = 1\n",
        "        while True:\n",
        "            poem_i = poems[poems['poem_id'] == i]\n",
        "          \n",
        "            index_i = poems.index[poems['poem_id'] == i]\n",
        "            current_poem = \"\"\n",
        "            for p in index_i:\n",
        "                v_position = poem_i.loc[p,\"v_position\"]\n",
        "                verse =  poem_i.loc[p,\"poem_text\"]\n",
        "                current_poem += verse \n",
        "                if v_position == 0:\n",
        "                    current_poem += \" \\t\"\n",
        "                if v_position == 1:\n",
        "                    current_poem += \" \\n\"\n",
        "            poems_sequence.append(current_poem)\n",
        "            i += 1\n",
        "            if len(poem_i)<1 :\n",
        "              break\n",
        "    return poems_sequence\n",
        "import os\n",
        "paths = os.listdir(\"/content/drive/MyDrive/Data/poems\")\n",
        "# print(paths)\n",
        "paths_arr = [\"/content/drive/MyDrive/Data/poems/\" + f for f in paths]\n",
        "d_load = Data_load(60,2)\n",
        "# poems= d_load.read_data([\"/content/drive/MyDrive/Data/train.csv\"]+paths_arr)\n",
        "poems= d_load.read_data( [\"/content/drive/MyDrive/Data/train.csv\"\n",
        "                        ])\n",
        "\n",
        "\n",
        "X , Y  = [], []\n",
        "# persian_poems = open(\"/content/drive/MyDrive/persian_poems_test.txt\",'w')\n",
        "# for poem in poems:\n",
        "#   for vers in poem.split(\"\\n\"):\n",
        "#     try:\n",
        "#       x,y = vers.split(\"\\t\")\n",
        "#       X.append(x.strip())\n",
        "#       Y.append(y.strip())\n",
        "#     except ValueError:\n",
        "#       continue\n",
        "# beyt\n",
        "for poem in poems:\n",
        "  verses =poem.split(\"\\n\") \n",
        "  for i in range(len(verses)-1):\n",
        "    try:\n",
        "      X.append(verses[i])\n",
        "      Y.append(verses[i+1])\n",
        "    except ValueError:\n",
        "      continue\n",
        "# for poem in poems:\n",
        "#   index =poem.index(\"\\t\") \n",
        "#   X.append(poem[:index])\n",
        "#   Y.append(poem[index:])\n",
        "   "
      ],
      "metadata": {
        "id": "Fnr3dz1P8IUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osc729oi5U1w"
      },
      "source": [
        "df = pd.DataFrame({\"poet\":X[:60000],\"poem\":Y[:60000]})\n",
        "df[\"text\"] =df[\"poet\"] + \"<sep>\" + df[\"poem\"]\n",
        "df = df.reset_index(drop=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQVeZMuMGmkL"
      },
      "source": [
        "poets = list(df[\"poet\"].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbOSID0q8IU9"
      },
      "source": [
        "idx = np.random.randint(0, len(df))\n",
        "print(idx)\n",
        "print(df.iloc[idx][\"poet\"])\n",
        "print(df.iloc[idx][\"poem\"])\n",
        "print(df.iloc[idx][\"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J351-BahDVeM"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho5oIfRjEHhh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelWithLMHead\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Config\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdnnYyAeEt5H"
      },
      "source": [
        "model_name_or_path = \"gpt2-medium\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    bos_token='<s>', \n",
        "    eos_token='</s>', \n",
        "    pad_token='<pad>',\n",
        "    unk_token='<unk>'\n",
        ")\n",
        "tokenizer.add_special_tokens({\n",
        "    \"bos_token\": '<s>',\n",
        "    \"eos_token\": '</s>', \n",
        "    \"pad_token\": '<pad>',\n",
        "    \"unk_token\": '<unk>'\n",
        "})\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    bos_token_id=tokenizer(\"<s>\")[\"input_ids\"][0], \n",
        "    eos_token_id=tokenizer(\"</s>\")[\"input_ids\"][0], \n",
        "    pad_token_id=tokenizer(\"<pad>\")[\"input_ids\"][0],\n",
        "    unk_token_id=tokenizer(\"<unk>\")[\"input_ids\"][0],\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Pv98AHjH4jF"
      },
      "source": [
        "texts = df[\"text\"].values.tolist()\n",
        "\n",
        "print(len(texts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19fApJsoIJA1"
      },
      "source": [
        "max_seq = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5m3YNOvIKoU"
      },
      "source": [
        "from torch.utils.data import Dataset  # this is the pytorch class import\n",
        "import torch\n",
        "torch.manual_seed(42)\n",
        "class MTGDataset(Dataset):\n",
        "\n",
        "    def __init__(self, txt_list, tokenizer, max_length=1024):\n",
        "\n",
        "        self.tokenizer = tokenizer  # the gpt2 tokenizer we instantiated\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "\n",
        "        for txt in txt_list:\n",
        "            \"\"\"\n",
        "            This loop will iterate through each entry in the flavour text corpus.\n",
        "            For each bit of text it will prepend it with the start of text token,\n",
        "            then append the end of text token and pad to the maximum length with the \n",
        "            pad token. \n",
        "            \"\"\"\n",
        "\n",
        "            encodings_dict = tokenizer('<s>' + txt + '</s>',\n",
        "                                       truncation=True,\n",
        "                                       max_length=max_length,\n",
        "                                       padding=\"max_length\")\n",
        "\n",
        "            \"\"\"\n",
        "            Each iteration then appends either the encoded tensor to a list,\n",
        "            or the attention mask for that encoding to a list. The attention mask is\n",
        "            a binary list of 1's or 0's which determine whether the langauge model\n",
        "            should take that token into consideration or not. \n",
        "            \"\"\"\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwY3srMLINfs"
      },
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "dataset = MTGDataset(texts, tokenizer, max_length=max_seq)\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "f'There are {len(train_dataset)} samples for training, and {len(val_dataset)} samples for validation testing'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lplxA4inIReU"
      },
      "source": [
        "print(train_dataset[0][0][:100])\n",
        "print(tokenizer.decode(train_dataset[5][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hxgySomITYV"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    sampler=RandomSampler(train_dataset),\n",
        "    batch_size=8\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    sampler=SequentialSampler(val_dataset),\n",
        "    batch_size=8\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVUEzpPvIYYV"
      },
      "source": [
        "import random\n",
        "from transformers import GPT2LMHeadModel, GPT2Config\n",
        "import numpy as np\n",
        "\n",
        "# Loading the model configuration and setting it to the GPT2 standard settings.\n",
        "configuration = GPT2Config.from_pretrained('gpt2-medium', output_hidden_states=False)\n",
        "\n",
        "# Create the instance of the model and set the token size embedding length\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", config=configuration)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "device = torch.device(\"cuda\")\n",
        "model.cuda()\n",
        "\n",
        "# This step is optional but will enable reproducible runs.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zgo4J21IaYc"
      },
      "source": [
        "epochs = 3\n",
        "warmup_steps = 1e2\n",
        "sample_every = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0C5AtZ8IdWa"
      },
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# AdamW is a class from the huggingface library, it is the optimizer we will be using, and we will only be instantiating it with the default parameters.\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=5e-4,\n",
        "    eps=1e-8\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiWGu9MsIcw8"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\"\"\"\n",
        "Total training steps is the number of data points, times the number of epochs. \n",
        "Essentially, epochs are training cycles, how many times each point will be seen by the model. \n",
        "\"\"\"\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "\"\"\"\n",
        "We can set a variable learning rate which will help scan larger areas of the \n",
        "problem space at higher LR earlier, then fine tune to find the exact model minima \n",
        "at lower LR later in training.\n",
        "\"\"\"\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X)"
      ],
      "metadata": {
        "id": "sFqx_rHEMehX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-X-YWgdIgcT"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
        "\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch_i in tqdm(range(2), position=0):\n",
        "\n",
        "    print(f'Beginning epoch {epoch_i + 1} of {epochs}')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), position=0):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks, token_type_ids=None)\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        # Get sample every 100 batches.\n",
        "        # if step % sample_every == 0 and not step == 0:\n",
        "\n",
        "        #     elapsed = format_time(time.time() - t0)\n",
        "        #     print()\n",
        "        #     print(f'Batch {step} of {len(train_dataloader)}. Loss:{batch_loss}. Time:{elapsed}')\n",
        "\n",
        "        #     model.eval()\n",
        "\n",
        "        #     sample_poet = poets[np.random.randint(0, len(poets))]\n",
        "        #     sample_input = f\"<s>{sample_poet}<|startoftext|>\"\n",
        "        #     sample_input_ids = torch.tensor(tokenizer([sample_input])[\"input_ids\"])\n",
        "        #     sample_input_ids = sample_input_ids.to(device)\n",
        "\n",
        "        #     sample_outputs = model.generate(\n",
        "        #         input_ids=sample_input_ids,\n",
        "        #         # bos_token_id=random.randint(1, len(tokenizer.get_vocab())),\n",
        "        #         do_sample=True,\n",
        "        #         top_k=50,\n",
        "        #         max_length=50,\n",
        "        #         top_p=0.95,\n",
        "        #         num_return_sequences=1\n",
        "        #     )\n",
        "        #     for i, sample_output in enumerate(sample_outputs):\n",
        "        #         gen_sample_output = tokenizer.decode(sample_output, skip_special_tokens=False)\n",
        "        #         gen_sample_output = gen_sample_output.replace(\"<|startoftext|>\", \"\\n\")\n",
        "        #         gen_sample_output = gen_sample_output.replace(\"<s>\", \"\")\n",
        "        #         gen_sample_output = gen_sample_output.replace(\"</s>\", \"\")\n",
        "        #         gen_sample_output = gen_sample_output.replace(\"<sep>\", \"\\n\")\n",
        "\n",
        "        #         print(f'Example output: {gen_sample_output}')\n",
        "\n",
        "            # model.train()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print()\n",
        "    print(f'Average Training Loss: {avg_train_loss}. Epoch time: {training_time}')\n",
        "    print()\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in tqdm(validation_dataloader, total=len(validation_dataloader), position=0):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs = model(b_input_ids, attention_mask=b_masks, labels=b_labels)\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_eval_loss += batch_loss\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print()\n",
        "    print(f'Validation loss: {avg_val_loss}. Validation Time: {validation_time}')\n",
        "    print()\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(f'Total training took {format_time(time.time()-total_t0)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzzFo1-zIhna"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "pd.set_option('precision', 2)\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7s2FziqJn1x"
      },
      "source": [
        "import os\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/gpt2-poetry_small'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "configuration.save_pretrained(output_dir)\n",
        "!cp /content/drive/MyDrive/gpt2/tokenizer.json /content/drive/MyDrive/gpt2-fa-poetry/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfI4iqewfXtX"
      },
      "source": [
        "# from transformers import TFAutoModelForCausalLM\n",
        "\n",
        "\n",
        "# tf_model = TFAutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/gpt2-fa-poetry/\", from_pt=True)\n",
        "# print(tf_model.summary())\n",
        "# tf_model.save_pretrained(\"/content/drive/MyDrive/gpt2-fa-poetry-tf\")\n",
        "# !cp /content/drive/MyDrive/gpt2-fa-poetry-tf/tf_model.h5 /content/gpt2-fa-poetry \n",
        "# !rm -rf /content/drive/MyDrive/gpt2-fa-poetry-tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnvDWyzXfA1m"
      },
      "source": [
        "!zip -r gpt2-fa-poetry.zip gpt2-fa-poetry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = '/content/drive/MyDrive/gpt2-poetry_small'\n",
        "configuration = GPT2Config.from_pretrained(model_dir, output_hidden_states=False)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_dir, config=configuration)\n",
        "# tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/gpt2-fa-poetry')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_dir,\n",
        "    bos_token='<s>', \n",
        "    eos_token='</s>', \n",
        "    pad_token='<pad>',\n",
        "    unk_token='<unk>'\n",
        ")\n",
        "tokenizer.add_special_tokens({\n",
        "    \"bos_token\": '<s>',\n",
        "    \"eos_token\": '</s>', \n",
        "    \"pad_token\": '<pad>',\n",
        "    \"unk_token\": '<unk>'\n",
        "})\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "  model_dir,\n",
        "    bos_token_id=tokenizer(\"<s>\")[\"input_ids\"][0], \n",
        "    eos_token_id=tokenizer(\"</s>\")[\"input_ids\"][0], \n",
        "    pad_token_id=tokenizer(\"<pad>\")[\"input_ids\"][0],\n",
        "    unk_token_id=tokenizer(\"<unk>\")[\"input_ids\"][0],\n",
        ")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ],
      "metadata": {
        "id": "fqPHMy3q5PGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh2FAX8CJueV"
      },
      "source": [
        "import re\n",
        "def generator(poet, max_length=20, num_return_sequences=3):\n",
        "    model.eval()\n",
        "    prompt = f\"<s>{poet}<sep>\"\n",
        "    # print(prompt)\n",
        "\n",
        "    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "    # generated = generated.to(device)\n",
        "\n",
        "    decoded_outputs = model.generate(\n",
        "        generated,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        max_length=100, \n",
        "        top_p=0.8,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        bos_token_id=tokenizer(\"<s>\")[\"input_ids\"][0], \n",
        "        eos_token_id=tokenizer(\"</s>\")[\"input_ids\"][0], \n",
        "        pad_token_id=tokenizer(\"<pad>\")[\"input_ids\"][0],\n",
        "        unk_token_id=tokenizer(\"<unk>\")[\"input_ids\"][0], \n",
        "    )\n",
        "    outputs = []\n",
        "    for i, output in enumerate(decoded_outputs):\n",
        "        o = tokenizer.decode(output, skip_special_tokens=False)\n",
        "        # print(o)\n",
        "        o = o.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\",\"\")\n",
        "        outputs.append(o)\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsB-3HZCNbaE"
      },
      "source": [
        "poems = pd.read_csv(\"/content/drive/MyDrive/Data/test.csv\")\n",
        "# print(poems['poem_id'])\n",
        "i = 3\n",
        "poems_sequence =[]\n",
        "while True:\n",
        "            poem_i = poems[poems['poem_id'] == i]\n",
        "            # print(poem_i)\n",
        "            index_i = poems.index[poems['poem_id'] == i]\n",
        "            current_poem = \"\"\n",
        "            for p in index_i:\n",
        "                v_position = poem_i.loc[p,\"v_position\"]\n",
        "                verse =  poem_i.loc[p,\"poem_text\"]\n",
        "                # print(verse)\n",
        "                current_poem += verse \n",
        "                if v_position == 0:\n",
        "                    current_poem += \" \\t\"\n",
        "                if v_position == 1:\n",
        "                    current_poem += \" \\n\"\n",
        "            if len(current_poem)>0:\n",
        "              poems_sequence.append(current_poem)\n",
        "            i += 1\n",
        "            if i>596 :\n",
        "              break\n",
        "# # print(poems)\n",
        "# X , Y  = [], []\n",
        "# for poem in poems_sequence:\n",
        "#   verses =poem.split(\"\\n\") \n",
        "#   for i in range(len(verses)-1):\n",
        "#     try:\n",
        "#       X.append(verses[i])\n",
        "#       Y.append(verses[i+1])\n",
        "#     except ValueError:\n",
        "#       continue\n",
        "X,Y = [],[]\n",
        "print(len(poems_sequence))\n",
        "for poem in poems_sequence:\n",
        "  # print(len(poem.split(\"\\n\")))\n",
        "  for vers in poem.split(\"\\n\"):\n",
        "    try:\n",
        "      x,y = vers.split(\"\\t\")\n",
        "      X.append(x.strip())\n",
        "      Y.append(y.strip())\n",
        "    except ValueError:\n",
        "      continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLEU"
      ],
      "metadata": {
        "id": "GNsjPNLdclU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
        "score1 = 0\n",
        "score2 = 0\n",
        "score3 = 0\n",
        "for x, y in zip(X[:500],Y[:500]):\n",
        "  outputs = generator(x, num_return_sequences=1)\n",
        "  print(x)\n",
        "  print(\"************\")\n",
        "  print(y)\n",
        "  print(\"******************\")\n",
        "  print(outputs[0][outputs[0].index(\"<sep>\"):])\n",
        "  print(\"#####################################\")\n",
        "  # print(outputs[0][outputs[0].index(\"<sep>\")+15:])\n",
        "  # bleu = BLEU(test_text=outputs[0][outputs[0].index(\"<|startoftext|>\")+15:],real_text=y,gram = 1)\n",
        "  # print(bleu.get_bleu())\n",
        "  s1 = sentence_bleu([f.split(\" \")for f in Y[:500]],outputs[0][outputs[0].index(\"<sep>\")+15:].split(\" \"),weights= (1,0,0,0)) \n",
        "  s2 = sentence_bleu([f.split(\" \")for f in Y[:500]],outputs[0][outputs[0].index(\"<sep>\")+15:].split(\" \"),weights= (0,1,0,0)) \n",
        "  s3 = sentence_bleu([f.split(\" \")for f in Y[:500]],outputs[0][outputs[0].index(\"<sep>\")+15:].split(\" \"),weights= (0,0,1,0)) \n",
        "  score1 += s1\n",
        "  score2 += s2\n",
        "  score3 += s3\n",
        "  print(s1,s2,s3)\n",
        "  print(\"****************************************\")"
      ],
      "metadata": {
        "id": "JY8KYiPj9u4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score1/500,score2/500, score3/500\n",
        "\n"
      ],
      "metadata": {
        "id": "vb2sIerHqmn-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}