{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU_project_12_2_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install parsivar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2tuw9tIacyF",
        "outputId": "e21c158f-725f-49dd-a421-c0e6544b772a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: parsivar in /usr/local/lib/python3.7/dist-packages (0.2.3)\n",
            "Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/dist-packages (from parsivar) (3.4.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->parsivar) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.randint(25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "083VqaoT_IOf",
        "outputId": "703c5803-07c1-4703-fcdc-612f56642f69"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6qTYlz6M1pbN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from parsivar import Tokenizer,FindStems,Normalizer\n",
        "import os \n",
        "import re \n",
        "import torch \n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEtkQm9TyUYF",
        "outputId": "e808dc78-9e94-4d83-bbf5-ee52fa071020"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "  def __init__(self,batch_size,hidden_dim,vocab_l,window):\n",
        "    self.batch_size = batch_size\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.vocab_l = vocab_l\n",
        "    self.window = window\n",
        "    self.dis_init = \"uniform\"\n",
        "    self.gen_init =\"uniform\"\n",
        "    if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
        "        self.gpu = int(True)\n",
        "    else:\n",
        "        self.gpu = int(False)\n"
      ],
      "metadata": {
        "id": "SwI_5vLfT6aF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data_load:\n",
        "  def __init__(self,max_length, step):\n",
        "    # self.train_path = train_path\n",
        "    # self.test_path = test_path\n",
        "    # self.test_sequence = self.read_data(test_path)\n",
        "    self.vocab = []\n",
        "    # self.voocab_l = len(self.vocab)\n",
        "    self.max_length = max_length\n",
        "    self.step = step\n",
        "    self.load_dictionary()\n",
        "\n",
        "  def load_dictionary(self):\n",
        "    words = open(\"/content/drive/MyDrive//NLU_Project/vocab.txt\").read().split(\"\\n\")  \n",
        "    for w in words:\n",
        "      self.vocab.append(w)\n",
        "    # self.vocab.append(\"\\n\")    \n",
        "    # self.vocab.append(\"\\t\")  \n",
        "    self.vocab.append(\"eos\")\n",
        "    self.vocab.append(\"sep\")\n",
        "    self.vocab.append(\"PAD\")\n",
        "    self.vocab.append(\"UNK\")  \n",
        "    self.vocab.append(\"BOS\")  \n",
        "    self.vocab = sorted(self.vocab)\n",
        "    self.dict_word2id = { word :i   for i,word in enumerate(self.vocab)}\n",
        "    self.dict_id2word = { i : word  for i,word in enumerate(self.vocab)}\n",
        "    # self.dict_word2id ['PAD'] = 0\n",
        "    # self.dict_word2id [\"UNK\"] = 2\n",
        "    # self.dict_word2id [\"BOS\"] = 1\n",
        "    # self.dict_word2id [\"sep\"] = 3\n",
        "    # self.dict_word2id [\"eos\"] = 4\n",
        "    # self.dict_id2word[0]=\"PAD\"\n",
        "    # self.dict_id2word[2] =\"UNK\"\n",
        "    # self.dict_id2word[1] =\"BOS\"\n",
        "    # self.dict_id2word[3] =\"sep\"\n",
        "    # self.dict_id2word[4] =\"eos\"\n",
        "    self.vocab_l = len(self.vocab)\n",
        "    \n",
        "  def create_sequence_data(self,paths):\n",
        "    poems =  self.read_data(paths)\n",
        "    all_poem_seq = []\n",
        "    for poem_seq in poems:\n",
        "        word2id_sequence = []\n",
        "        words_list = poem_seq.split(\" \")\n",
        "        # print(words_list)\n",
        "        for word in words_list:\n",
        "\n",
        "          try:\n",
        "              word2id_sequence.append(self.dict_word2id[word])\n",
        "          except KeyError:\n",
        "              word2id_sequence.append(self.dict_word2id[\"UNK\"])\n",
        "        all_poem_seq.append(word2id_sequence)\n",
        "    X_gen = []\n",
        "    X_dis = []\n",
        "    Y = []\n",
        "    for poem_seq in all_poem_seq:\n",
        "      for i in range(self.max_length,len(poem_seq),self.step):\n",
        "         X_gen.append([1]+poem_seq[i - self.max_length: i-1])\n",
        "         X_dis.append(poem_seq[i - self.max_length: i])\n",
        "         Y.append(poem_seq[i - self.max_length+1: i+1])\n",
        "    return X_gen, X_dis, Y\n",
        "            \n",
        "  def read_data(self,paths):\n",
        "    poems_sequence = []\n",
        "    poems = pd.read_csv(paths[0])\n",
        "    i = 1\n",
        "    while True:\n",
        "            poem_i = poems[poems['poem_id'] == i]\n",
        "            # print(poem_i)\n",
        "            index_i = poems.index[poems['poem_id'] == i]\n",
        "            current_poem = \"\"\n",
        "            for p in index_i:\n",
        "                v_position = poem_i.loc[p,\"v_position\"]\n",
        "                verse =  poem_i.loc[p,\"poem_text\"]\n",
        "                # print(verse)\n",
        "                current_poem += verse \n",
        "                if v_position == 0:\n",
        "                    current_poem += \" sep\"\n",
        "                if v_position == 1:\n",
        "                    current_poem += \" eos\"\n",
        "            if len(current_poem)>0:\n",
        "              poems_sequence.append(current_poem)\n",
        "            i += 1\n",
        "            if i>599 :\n",
        "              break\n",
        "    for path in paths[1:]:\n",
        "        \n",
        "        poems = pd.read_csv(path)\n",
        "        # print(poems['poem_id'])\n",
        "        i = 1\n",
        "        while True:\n",
        "            poem_i = poems[poems['poem_id'] == i]\n",
        "          \n",
        "            index_i = poems.index[poems['poem_id'] == i]\n",
        "            current_poem = \"\"\n",
        "            for p in index_i:\n",
        "                v_position = poem_i.loc[p,\"v_position\"]\n",
        "                verse =  poem_i.loc[p,\"poem_text\"]\n",
        "                current_poem += verse \n",
        "                if v_position == 0:\n",
        "                    current_poem += \" sep\"\n",
        "                if v_position == 1:\n",
        "                    current_poem += \" eos\"\n",
        "            poems_sequence.append(current_poem)\n",
        "            i += 1\n",
        "            if len(poem_i)<1 :\n",
        "              break\n",
        "    return poems_sequence"
      ],
      "metadata": {
        "id": "HT1d0slv5I6U"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import os\n",
        "import pandas as pd\n",
        "paths = os.listdir(\"/content/drive/MyDrive/NLU_Project/Data/poems\")\n",
        "paths_arr = [\"/content/drive/MyDrive/NLU_Project/Data/poems/\" + f for f in paths]\n",
        "d_load = Data_load(30,1)\n",
        "# poems_list = d_load.read_data([\"/content/drive/MyDrive/NLU_Project/Data/train.csv\"]+paths_arr)\n",
        "# poems_list = d_load.read_data([\"/content/drive/MyDrive/NLU_Project/Data/train.csv\"])\n",
        "args = Args(32,40,d_load.vocab_l,d_load.max_length)"
      ],
      "metadata": {
        "id": "_JwbOgeD5scB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dff8556-d3a8-4d51-ca99-6e8a7e8d0038"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 745 Âµs, sys: 3.03 ms, total: 3.78 ms\n",
            "Wall time: 9.81 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(d_load.dict_id2word.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXj_Y1IS7Dkx",
        "outputId": "52e3e9ff-dd6b-4e75-88ee-1b1d33e37b7b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1817"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # from parsivar import Normalizer\n",
        "# import re\n",
        "# from parsivar import Tokenizer\n",
        "# my_normalizer = Normalizer()\n",
        "# my_tokenizer = Tokenizer()\n",
        "# words = {}\n",
        "# for poems in poems_list:\n",
        "#   poems = re.sub(\"[0-9a-z]|\\!|\\?|\\.|\\Ø\",\"\",poems)\n",
        "#   tokens = my_tokenizer.tokenize_words(my_normalizer.normalize(poems))\n",
        "#   for token in tokens:\n",
        "#     try:\n",
        "#        words[token] +=1\n",
        "#     except KeyError:\n",
        "#       words[token] =1"
      ],
      "metadata": {
        "id": "QWmMHkClPmvB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f = open(\"/content/drive/MyDrive/NLU_Project/vocab_text.txt\",'w')\n",
        "# for w in list(words.keys()):\n",
        "#     f.write(w)\n",
        "#     f.write(\"\\n\")\n",
        "# f.close()"
      ],
      "metadata": {
        "id": "sSge6ZhKUi-F"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_gen,X_dis,Y= d_load.create_sequence_data( [\n",
        "                                             \"/content/drive/MyDrive/NLU_Project/Data/train.csv\",\n",
        "                                             \"/content/drive/MyDrive/NLU_Project/Data/poems/Ø³Ø¹Ø¯Û.csv\"])\n",
        "len(X_gen)"
      ],
      "metadata": {
        "id": "nC9DQ6zetGis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d77b3ae7-2f19-4bce-8388-f4e6d0ca8f44"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "206162"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "dataset_gen = TensorDataset(torch.tensor(X_gen[:10000]),torch.tensor(Y[:10000]))\n",
        "dataloader_gen = DataLoader(dataset_gen, batch_size=args.batch_size)\n",
        "dataset_dis = TensorDataset(torch.tensor(X_dis[:10000]),torch.tensor(Y[:10000]))\n",
        "dataloader_dis = DataLoader(dataset_dis, batch_size=args.batch_size)"
      ],
      "metadata": {
        "id": "rqX68TNNA9sf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f = open(\"/content/drive/MyDrive/NLU_Project/vocab_text.txt\",\"w\")\n",
        "# for i in d_load.vocab:\n",
        "#   f.write(i)\n",
        "#   f.write(\"\\n\")\n",
        "# f.close()"
      ],
      "metadata": {
        "id": "HBQ8NNjf5qzl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        " \n",
        "\n",
        "class Metrics:\n",
        "    __metaclass__ = abc.ABCMeta\n",
        "    def __init__(self, name='Metric'):\n",
        "        self.name = name\n",
        "\n",
        "    def get_name(self):\n",
        "        return self.name\n",
        "\n",
        "    def set_name(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def get_score(self):\n",
        "        pass\n",
        "\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "class NLL(Metrics):\n",
        "    def __init__(self, name, model, loader, gpu=True):\n",
        "        super(NLL, self).__init__(name)\n",
        "\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.gpu = gpu\n",
        "        self.need_reset = True\n",
        "\n",
        "    def get_score(self, model=None, loader=None, ignore=False):\n",
        "        \"\"\"note that NLL score need the updated model and data loader each time, use reset() before get_score()\"\"\"\n",
        "        if ignore:\n",
        "            return 0\n",
        "        if model and loader:\n",
        "            self.reset(model, loader)\n",
        "        assert not self.need_reset, 'need reset model and loader before calculating NLL'\n",
        "        self.need_reset = True\n",
        "        return self.cal_nll()\n",
        "\n",
        "    def reset(self, model, loader):\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.need_reset = False\n",
        "\n",
        "    def cal_nll(self,model):\n",
        "        total_loss = 0\n",
        "        criterion = nn.NLLLoss()\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.loader):\n",
        "                inp, target = data\n",
        "                if self.gpu:\n",
        "                    inp, target = inp.cuda(), target.cuda()\n",
        "                h,c = self.model.init_state(inp.size()[0])\n",
        "                pred,_, _, _ = model.forward(inp, (h.to(device),c.to(device)))\n",
        "                for k in range(inp.size(1)):\n",
        "                    loss = criterion(pred[:,k,:], target[:,k].view(-1))\n",
        "                    total_loss += loss.item()\n",
        "                # print(torch.argmax(pred,dim=2).size(),target.view(-1).size())\n",
        "                \n",
        "        return total_loss / len(self.loader)"
      ],
      "metadata": {
        "id": "aLfEYdrtpIJa"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from posix import X_OK\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.hidden_dim =200\n",
        "        self.embedding_dim = 300\n",
        "        self.num_layers = 3\n",
        "        self.temperature = 0.1\n",
        "        n_vocab = args.vocab_l\n",
        "        self.embeddings = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "           self.embedding_dim, self.hidden_dim, batch_first=True,\n",
        "           \n",
        "        )\n",
        "        self.fc = nn.Linear(self.hidden_dim, n_vocab)\n",
        "        # self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        for param in self.parameters():\n",
        "            if param.requires_grad and len(param.shape) > 0:\n",
        "                stddev = 1 / math.sqrt(param.shape[0])\n",
        "                if args.dis_init == 'uniform':\n",
        "                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n",
        "                elif args.dis_init == 'normal':\n",
        "                    torch.nn.init.normal_(param, std=stddev)\n",
        "    def forward(self, inp, hidden,need_hidden= False):\n",
        "        self.theta = []\n",
        "        if args.gpu:\n",
        "                inp = inp.cuda()\n",
        "        emb = self.embeddings(inp)  # batch_size * len * embedding_dim\n",
        "        if len(inp.size()) == 1:\n",
        "                   emb = emb.unsqueeze(1)  # batch_size * 1 * embedding_dim\n",
        "        out, hidden = self.lstm(emb, hidden)\n",
        "        out = self.fc(out.squeeze(1)) \n",
        "        out = F.softmax(out, dim=-1)\n",
        "        # out.retain_grad()\n",
        "        self.theta.append(out)\n",
        "        gumbel_t = self.add_gumbel(out)\n",
        "        next_token = torch.argmax(gumbel_t*self.temperature, dim=1)\n",
        "        pred = F.softmax(gumbel_t * self.temperature, dim=-1)  # batch_size * vocab_size\n",
        "        next_token_onehot = None\n",
        "        return pred, hidden, next_token, next_token_onehot\n",
        "       \n",
        "\n",
        "    def sample_generate(self,inp,hidden,one_hot=False):\n",
        "      inp =torch.randint(0,args.vocab_l,(inp.size()[0],)).long()\n",
        "      batch_size = inp.size()[0]   \n",
        "      samples = torch.zeros( inp.size()[0], args.window).long()\n",
        "      if one_hot:\n",
        "            all_preds = torch.zeros( inp.size()[0], args.window, args.vocab_l)\n",
        "            if args.gpu:\n",
        "                all_preds = all_preds.cuda()\n",
        "      for i in range(args.window):\n",
        "                pred, hidden, next_token, next_token_onehot = self.forward(inp,hidden) \n",
        "                if one_hot:\n",
        "                    all_preds[:, i] = pred\n",
        "                samples[0:batch_size, i] = next_token\n",
        "      if one_hot:\n",
        "            return all_preds  # batch_size * seq_len * vocab_size\n",
        "      return samples\n",
        "\n",
        "\n",
        "    def add_gumbel(self,theta, eps=1e-10, gpu=args.gpu):\n",
        "        u = torch.zeros(theta.size())\n",
        "        if gpu:\n",
        "            u = u.cuda()\n",
        "        u.uniform_(0, 1)\n",
        "        gumbel_t = torch.log(theta + eps) - torch.log(-torch.log(u + eps) + eps)\n",
        "        return gumbel_t \n",
        "\n",
        "\n",
        "    def init_state(self, batch_size):\n",
        "        h = torch.zeros(1, batch_size, self.hidden_dim)\n",
        "        c = torch.zeros(1, batch_size, self.hidden_dim)\n",
        "        return h, c\n",
        "     \n",
        "dis_filter_sizes = [10, 15, 20, 25, 30, 4, 5, 2]\n",
        "dis_num_filters = [50, 50, 80, 80, 80, 50, 50, 50]\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "   def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # self.lstm_size = 128\n",
        "        self.embedding_dim = 300\n",
        "        self.num_layers = 3\n",
        "        self.feature_dim = sum(dis_num_filters)\n",
        "        # self.gpu = gpu\n",
        "        n_vocab = args.vocab_l\n",
        "        # self.embeddings = nn.Embedding(\n",
        "        #     num_embeddings=n_vocab,\n",
        "        #     embedding_dim=self.embedding_dim,\n",
        "        # )\n",
        "        self.embeddings = nn.Linear(args.vocab_l, self.embedding_dim, bias=False)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, n, (f, self.embedding_dim)) for (n, f) in zip(dis_num_filters , dis_filter_sizes)\n",
        "        ])\n",
        "        self.highway = nn.Linear(self.feature_dim, self.feature_dim)\n",
        "        self.feature2out = nn.Linear(self.feature_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.init_params()\n",
        "\n",
        "   def forward(self, x):\n",
        "        feature = self.get_feature(x)\n",
        "        out = self.feature2out(self.dropout(feature))\n",
        "        pred = nn.Sigmoid()(out.reshape(out.size()[0]))\n",
        "        return pred\n",
        "\n",
        "   def get_feature(self, inp):\n",
        "        emb = self.embeddings(inp).unsqueeze(1) # batch_size * 1 * max_seq_len * embed_dim\n",
        "        # print(emb.size(),\"*********\")\n",
        "        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs]  # [batch_size * num_filter * length]\n",
        "        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs]  # [batch_size * num_filter]\n",
        "        pred = torch.cat(pools, 1)  # tensor: batch_size * feature_dim\n",
        "        highway = self.highway(pred)\n",
        "        pred = torch.sigmoid(highway) * F.relu(highway) + (1. - torch.sigmoid(highway)) * pred  # highway\n",
        "        return pred\n",
        "\n",
        "   def init_params(self):\n",
        "        for param in self.parameters():\n",
        "            if param.requires_grad and len(param.shape) > 0:\n",
        "                stddev = 1 / math.sqrt(param.shape[0])\n",
        "                if args.dis_init == 'uniform':\n",
        "                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n",
        "                elif args.dis_init == 'normal':\n",
        "                    torch.nn.init.normal_(param, std=stddev)"
      ],
      "metadata": {
        "id": "OS1TMDigtTae"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from abc import abstractmethod\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import optim"
      ],
      "metadata": {
        "id": "9swOJhUp6Jn5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "class SGD(Optimizer):\n",
        "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False, *, maximize=False, foreach: Optional[bool] = None):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov,\n",
        "                        maximize=maximize, foreach=foreach)\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super().__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "            group.setdefault('maximize', False)\n",
        "            group.setdefault('foreach', None)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad = []\n",
        "            d_p_list = []\n",
        "            momentum_buffer_list = []\n",
        "            has_sparse_grad = False\n",
        "            i = 0 \n",
        "            for p in group['params']:\n",
        "               \n",
        "                if p.grad is not None:\n",
        "                    i += 1\n",
        "                    params_with_grad.append(p)\n",
        "                    d_p_list.append(p.grad)\n",
        "                    if p.grad.is_sparse:\n",
        "                        has_sparse_grad = True\n",
        "\n",
        "                    state = self.state[p]\n",
        "                    if 'momentum_buffer' not in state:\n",
        "                        momentum_buffer_list.append(None)\n",
        "                    else:\n",
        "                        momentum_buffer_list.append(state['momentum_buffer'])\n",
        "\n",
        "            sgd(params_with_grad,\n",
        "                d_p_list,\n",
        "                momentum_buffer_list,\n",
        "                weight_decay=group['weight_decay'],\n",
        "                momentum=group['momentum'],\n",
        "                lr=group['lr'],\n",
        "                dampening=group['dampening'],\n",
        "                nesterov=group['nesterov'],\n",
        "                maximize=group['maximize'],\n",
        "                has_sparse_grad=has_sparse_grad,\n",
        "                foreach=group['foreach'])\n",
        "\n",
        "            # update momentum_buffers in state\n",
        "            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
        "                state = self.state[p]\n",
        "                state['momentum_buffer'] = momentum_buffer\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "def sgd(params: List[Tensor],\n",
        "        d_p_list: List[Tensor],\n",
        "        momentum_buffer_list: List[Optional[Tensor]],\n",
        "        # kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\n",
        "        # setting this as kwarg for now as functional API is compiled by torch/distributed/optim\n",
        "        has_sparse_grad: bool = None,\n",
        "        foreach: bool = None,\n",
        "        *,\n",
        "        weight_decay: float,\n",
        "        momentum: float,\n",
        "        lr: float,\n",
        "        dampening: float,\n",
        "        nesterov: bool,\n",
        "        maximize: bool):\n",
        "    r\"\"\"Functional API that performs SGD algorithm computation.\n",
        "\n",
        "    See :class:`~torch.optim.SGD` for details.\n",
        "    \"\"\"\n",
        "\n",
        "    if foreach is None:\n",
        "        # Placeholder for more complex foreach logic to be added when value is not set\n",
        "        foreach = False\n",
        "\n",
        "    if foreach and torch.jit.is_scripting():\n",
        "        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n",
        "\n",
        "    if foreach and not torch.jit.is_scripting():\n",
        "        func = _multi_tensor_sgd\n",
        "    else:\n",
        "        func = _single_tensor_sgd\n",
        "    # print( func )\n",
        "    func(params,\n",
        "         d_p_list,\n",
        "         momentum_buffer_list,\n",
        "         weight_decay=weight_decay,\n",
        "         momentum=momentum,\n",
        "         lr=lr,\n",
        "         dampening=dampening,\n",
        "         nesterov=nesterov,\n",
        "         has_sparse_grad=has_sparse_grad,\n",
        "         maximize=maximize)\n",
        "\n",
        "def _single_tensor_sgd(params: List[Tensor],\n",
        "                       d_p_list: List[Tensor],\n",
        "                       momentum_buffer_list: List[Optional[Tensor]],\n",
        "                       *,\n",
        "                       weight_decay: float,\n",
        "                       momentum: float,\n",
        "                       lr: float,\n",
        "                       dampening: float,\n",
        "                       nesterov: bool,\n",
        "                       maximize: bool,\n",
        "                       has_sparse_grad: bool):\n",
        "\n",
        "    for i, param in enumerate(params):\n",
        "\n",
        "        d_p = d_p_list[i]\n",
        "        if weight_decay != 0:\n",
        "            d_p = d_p.add(param, alpha=weight_decay)\n",
        "\n",
        "        if momentum != 0:\n",
        "            buf = momentum_buffer_list[i]\n",
        "\n",
        "            if buf is None:\n",
        "                buf = torch.clone(d_p).detach()\n",
        "                momentum_buffer_list[i] = buf\n",
        "            else:\n",
        "                buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
        "\n",
        "            if nesterov:\n",
        "                d_p = d_p.add(buf, alpha=momentum)\n",
        "            else:\n",
        "                d_p = buf\n",
        "\n",
        "        alpha = lr if maximize else -lr\n",
        "        # pre_p = param.clone()\n",
        "        param.add_(d_p, alpha=alpha)\n",
        "        # print(pre_p - param)\n",
        "\n",
        "\n",
        "def _multi_tensor_sgd(params: List[Tensor],\n",
        "                      grads: List[Tensor],\n",
        "                      momentum_buffer_list: List[Optional[Tensor]],\n",
        "                      *,\n",
        "                      weight_decay: float,\n",
        "                      momentum: float,\n",
        "                      lr: float,\n",
        "                      dampening: float,\n",
        "                      nesterov: bool,\n",
        "                      maximize: bool,\n",
        "                      has_sparse_grad: bool):\n",
        "\n",
        "    if len(params) == 0:\n",
        "        return\n",
        "\n",
        "    if has_sparse_grad is None:\n",
        "        has_sparse_grad = any([grad.is_sparse for grad in grads])\n",
        "\n",
        "    if weight_decay != 0:\n",
        "        grads = torch._foreach_add(grads, params, alpha=weight_decay)\n",
        "\n",
        "    if momentum != 0:\n",
        "        bufs = []\n",
        "\n",
        "        all_states_with_momentum_buffer = True\n",
        "        for i in range(len(momentum_buffer_list)):\n",
        "            if momentum_buffer_list[i] is None:\n",
        "                all_states_with_momentum_buffer = False\n",
        "                break\n",
        "            else:\n",
        "                bufs.append(momentum_buffer_list[i])\n",
        "\n",
        "        if all_states_with_momentum_buffer:\n",
        "            torch._foreach_mul_(bufs, momentum)\n",
        "            torch._foreach_add_(bufs, grads, alpha=1 - dampening)\n",
        "        else:\n",
        "            bufs = []\n",
        "            for i in range(len(momentum_buffer_list)):\n",
        "                if momentum_buffer_list[i] is None:\n",
        "                    buf = momentum_buffer_list[i] = torch.clone(grads[i]).detach()\n",
        "                else:\n",
        "                    buf = momentum_buffer_list[i]\n",
        "                    buf.mul_(momentum).add_(grads[i], alpha=1 - dampening)\n",
        "\n",
        "                bufs.append(buf)\n",
        "\n",
        "        if nesterov:\n",
        "            torch._foreach_add_(grads, bufs, alpha=momentum)\n",
        "        else:\n",
        "            grads = bufs\n",
        "\n",
        "    alpha = lr if maximize else -lr\n",
        "    if not has_sparse_grad:\n",
        "        torch._foreach_add_(params, grads, alpha=alpha)\n",
        "    else:\n",
        "        # foreach APIs dont support sparse\n",
        "        for i in range(len(params)):\n",
        "            params[i].add_(grads[i], alpha=alpha)"
      ],
      "metadata": {
        "id": "e6R7tcrJjkD5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netD = Discriminator()\n",
        "netG = Generator()\n",
        "optimizerD = SGD(netD.parameters(), lr = 0.0001)\n",
        "optimizerG =SGD(netG.parameters(), lr = 0.0001)\n",
        "# print(netG.parameters())\n",
        "criterion = nn.BCELoss()\n",
        "nllloss = NLL(\"NLL\", netG, dataloader_dis, gpu=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "netD.to(device)\n",
        "netG.to(device) \n",
        "gloss = []\n",
        "dloss = []\n",
        "Nloss = []\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  print(epoch)\n",
        "  i = 0\n",
        "  for data_gen,data_dis in zip(dataloader_gen,dataloader_dis):\n",
        "    optimizerD.zero_grad()\n",
        "    optimizerG.zero_grad()\n",
        "    # for p1,p2 in zip(pre_param, netG.parameters()):\n",
        "    #   print(p1.grad == 0)\n",
        "    real, _ = data_dis\n",
        "    d_gen, _ = data_gen\n",
        "    input_g = Variable(d_gen).to(device)\n",
        "    input = Variable(real).to(device)\n",
        "    target = Variable(torch.ones(input.size()[0])).to(device)\n",
        "    state_h, state_c = netG.init_state(input_g.size()[0])\n",
        "    gen_samples = netG.sample_generate(input_g, (state_h.to(device), state_c.to(device)), one_hot=True)\n",
        "    if args.gpu:\n",
        "       real, gen_samples = real.cuda(), gen_samples.cuda()\n",
        "    real_samples = F.one_hot(real, args.vocab_l).float()\n",
        "    # real error for discriminator\n",
        "    output =  torch.reshape(netD(real_samples),(-1,)).to(device)\n",
        "    errD_real = criterion(output, target)\n",
        "    ###################################################################\n",
        "    output = netD(gen_samples)\n",
        "    # print(output)\n",
        "    target = Variable(torch.zeros(input_g.size()[0])).to(device)\n",
        "    errD_fake = criterion(torch.reshape(output, (-1,)), target)\n",
        "    errD = errD_real + errD_fake\n",
        "    errD.backward(retain_graph=True)\n",
        "    optimizerD.step()\n",
        "    # optimize Generator\n",
        "    target = Variable(torch.ones(input.size()[0])).to(device)\n",
        "    output = netD(gen_samples)\n",
        "    # print(output)\n",
        "    errG = criterion(torch.reshape(output, (-1,)), target)\n",
        "    # print(errG)\n",
        "    errG.backward()\n",
        "    pre_param = []\n",
        "   \n",
        "    if netG is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(netG.parameters(), 5)\n",
        "    optimizerG.step()\n",
        "    if i % 100== 0:\n",
        "      nlls = nllloss.cal_nll(netG)\n",
        "      print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f NLL_Loss:%4f' % (epoch, epochs, i, len(dataloader_gen), errD.detach(), errG.detach(),nlls))\n",
        "      gloss.append(errG.cpu().data.numpy())\n",
        "      dloss.append(errD.cpu().data.numpy())\n",
        "      Nloss.append(nlls)\n",
        "      #vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True)\n",
        "      state_h, state_c = netG.init_state(input_g.size()[0])\n",
        "      fake = netG.sample_generate(input_g.float(),(state_h.to(device), state_c.to(device)))\n",
        "      # print(fake[0,:])\n",
        "      str1 = \"\"\n",
        "      for o in fake:\n",
        "        str1 = \"\"\n",
        "        for w in o:  \n",
        "          # print(w)     \n",
        "          str1 += d_load.vocab[w]\n",
        "          str1 +=\" \"\n",
        "        print(str1)\n",
        "        break\n",
        "    i += 1\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(gloss)\n",
        "plt.plot(dloss)\n",
        "plt.plot(Nloss)\n",
        "plt.show()\n",
        "      #vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True)"
      ],
      "metadata": {
        "id": "aqDb0L95JC9k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56ad0f3b-5a92-45d5-9e6e-422258762791"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "0\n",
            "[0/10][0/313] Loss_D: 1.3638 Loss_G: 0.6652 NLL_Loss:-0.016518\n",
            "Ú¯Ø±ÙØ§ Ø­Ø±Øµ Ø¨ÛâÙÙØªÙØ§ ØªØ´ÙÙ Ø¯Ø±Ø¬ Ø¨Ø±ÙØªÛ Ø¨Ø³ØªØ§Ù Ø¨Ø±ÛÙ Ø¨Ø³Û ÚÛÙ Ø¨Ø§Ø´ Ø³ÛÙ Ø¯Ø±ÛØ§Û Ø³Ù Ú¯ÙØªÛ Ø¨Ø±Ú¯âØªØ± Ø­Ø±Øµ Ø­Ø±Ú©Øª ÙØ¨ÛÙØ¯Ø´ Ú¯Ø°Ø´Øª Ù¾Ø§Ú©ÛØ²ÙâØ³Øª Ø´ÛÙØªÙâÛ Ø¨Ø§Ø²ÛÚÙâÛ Ú©Ø±ÛØ¬Û ÛÙÛÙØª ØµÙØ±Øª Ø®ÛØ§ÙØª Ù¾Ø§ Ø¢ÙØ§Øª Ø¨Ù¾ÙØ´ÛØ´ \n",
            "[0/10][100/313] Loss_D: 1.3665 Loss_G: 0.6650 NLL_Loss:-0.016522\n",
            "Ø´ÙØ¹ ÙÙâÚ¯Ø± Ø³Ù¾Ø§Ø±Ù ÙÛØ§Ø±Ø¯ Ø­Ø¨Ø§Ø¨ ÙØ¹ÙÙØ± ÙÙØ®Ø§ÙÙ ØªÙØ§ÙÛ ÙÚÚ¯Ø§Ù ÛÙÛÙØª Ø²ÙØ³ØªØ§Ù Ú©Ø§ÙØ±ÛÙØª Ø¨Ø±Ù Ø³Ø§ÛÙ ÙØ±ÛØ¨ÙØ¯Ù Ø³ÙØ§Ø¯Ø´ ÙØ·ÛÙ ÙÙØ±âØ¨ÙØ¯Û Ø¢Ø®Ø± Ø®Ø§Øµ Ø¨ÛâÙÛØ§Ø²Û ØªØ¨Ø§ÙÛ Ø®Ø§Ø³Øª ÙÙØ¸ Ø´Ø±Ø¨Øª Ù¾Ø±Ø§Ú©ÙØ¯Ù Ø´ÙØ¹ Ø«Ø±ÛØ§Ø³Øª Ø§ÙØµÙØ§ ØªÙØ²ÛÙ \n",
            "[0/10][200/313] Loss_D: 1.3647 Loss_G: 0.6647 NLL_Loss:-0.016511\n",
            "Ø®Ø¯Ù Ø¨Ø¨Ø§ÛØ¯ ÙØ² ÙØªØ­Ù Ø¨ÛâØ®Ø±Ø¯Ø§Ù ØºÙØ¯Ø§Ù Ú©Ø±Ø¯Ø§Ø± Ø¯Ø§Øº Ø´Ø§ÙÙ ÙÛØ§Ø¨ÙØ¯ Ú¯Ø³Ø§Ø± Ø¬Ø³ÙÙØ§ Ù¾Ø±Ø³ Ø³Ú©ÙÙ Ø´Ø³ØªÙ Ø¯ÙØ³Øª Ø®Ø§ÙÙâØ¯Ø§Ø± Ú©ÚØ¨ÛÙâØ´Ø¯ÙâØ¨Ø§ Ø±Ø³ÙØ§ Ø¹ÙØ¯ ØºÙØ¯Ø§Ù Ø³Ø¨Ø²Û Ø§Ø²Ù Ø±Ø³ØªÙÛ ÙØ§ Ú©Ø±Ø¯ÙØ¯ Ø±Ø³ÛØ¯ ÙÙØ§Ø± ÙØ²Ø§Ù Ø®ÙØ§ÙÙ \n",
            "[0/10][300/313] Loss_D: 1.3634 Loss_G: 0.6643 NLL_Loss:-0.016503\n",
            "Ø²Ø´ÙÙ ÙÚÚ¯Ø§Ù Ø¢ÙØ±ÛØ¯ ÙØ­ÙØª Ù¾ÙØ´ Ø¢Ø± ÙØ§ÙÙÙØ¯ÙØ¯ ÙÙÛÙÙØ§âØ¨ÙØ¯ Ú¯ÙØªÙ ÙØ­ÙÙ ØªØ¹ÙÛØ± Ø³Ú©Ù Ø²ÙØ¯ Ø¬Ø¨ÛÙ ÙØ±ÙØ²Û ÙÙØ§ Ú©Ø´Øª ØªØ§Ø¨Ø¯ ÚØªØ±Ø´ Ø¨Ø±ÙØ§ Ø§ÙÙØ§ Ø¬ÙØ³ ÙØ§Ù¾ÛØ¯Ø§ Ú©Ø±Ø¯ Ø¨ÛâÙÙØªÙØ§ Ø§Ø¸ÙØ§Ø± ØµØ¯Ø± ÙØ¬ÙÛØ¯ ÙØ±ÙØ¯ Ú©Ø³ \n",
            "1\n",
            "[1/10][0/313] Loss_D: 1.3575 Loss_G: 0.6646 NLL_Loss:-0.016515\n",
            "ÙØ±Ø¯Ø§Ù Ø§ÙÙÙ Ø¨ÙÙ Ø¯Ø§ÙÙØ¯ Ø¬ÙØ²Ø§ Ø³Ø¬Ø§Ø¯Ù ÙØ­Ø§Ø¨Ø§ Ø®ÙØ§Ø¨ ÙØ±ÛÙ Ú¯Ø±Ø¯ Ø¨Ø¨ÛÙ Ø®ÙØ¨Û Ø³Ø±Ø§ ÙÙÙ Ú©Ø§ÙÚ©ÙØ¯ Ø´Ø±ÛÙ ÙÙÙ Ø®Ø§ÙÙ Ø®Ø§Ø±Û Ø®ÙØ¯ÛØ¯ ÚØ´ÙØ´ Ø¨Ø§Ø²Û ÙÚ¯ÛØ± Ø¨ÛâÚ¯ÙÙ ÙÙØ§Ø¯Ù Ø¨Ø¯ÙÙØ± Ø¯ÛØ¯Ø§Ø± ÙÙØ§Ø¯ ÙØ±ÙØ§Ù Ú¯ÙÙØ±Ø§Ù \n",
            "[1/10][100/313] Loss_D: 1.3574 Loss_G: 0.6642 NLL_Loss:-0.016517\n",
            "Ø¨ÙÛ eos Ù¾Ø°ÛØ±Ø¯ ÚÙØ¯ ÙØ¹Ø¯Ù Ø¯ÛØ¨Ø§ ÙØ§Ù¾Ø³ÙØ¯Ù Ù¾ÛÙØ§Ù Ø¯Ø±âØ¨ÙØ¯ Ú©Ø´ØªÙ Ù¾ÙØ¬Ù ØªØ®ØµÛØµ ÙØ§Ú¯ÙØªÙ ÛØ§Ø±Û Ø¬ÙÛØ¯ ÚÙØ¯Ù Ø¢Ù ÙØ¹ÙØ§ ÙØµØ·ÙØ§ Ø¨Ø³ØªâÚ¯Ø±Ø¯Ù ÙÙØ´ Ø¨ÙØ§ÛÛ Ø¯ÙØªØ± ÙØ±Ú¯Ø² Ù¾Ø§ Ø±Ø§Û Ø­Ú©Ù Ú©ÚÚ© Ø²Ø¨ÙØ±Ù Ø¹Ø¸Ù \n",
            "[1/10][200/313] Loss_D: 1.3573 Loss_G: 0.6644 NLL_Loss:-0.016522\n",
            "Ø·Ù¾ÛØ¯ Ø³Ø±Ø§Ø¯ÙØ§Øª Ø²ÙØ¯Ù Ø«ÙØ§ Ø´Ø³ØªÙ Ø³Ø¬Ø§Ø¯Ù Ù¾Ø±ÙØ±Ø¯Ú¯Ø§Ø± ØµÙØ§ ÙÛØ±Ø§ÙÛ Ø´ÛÙØªÙâÛ Ø­ÙØ±Ø§Ù Ø§ÙÙØ§Ø¯ Ø³ÛÙÛÙ Ø¨ÛâØ¨Ø¯Ù Ù¾Ø§Ø¯Ø´Ø§ÛÛ Ø¯ÙØ¯Û Ø±ÙÛØ¨Øª Ú©Û ØµÙØ§Ø¹Øª ÙØ² Ø¯Ø±Ø¯ Ø¨ÛâÙ¾Ø±ÙØ§ ØµØ­Ø¨Øª ÙØ®Ù Ø¯Ø§ÙØ´ Ø³Ø®Ù ÙØ§Ø±Ø§ ÙÛØ§Ù ÙÙØ§ÛØ§ØªØ´ Ø®Ø´Ù \n",
            "[1/10][300/313] Loss_D: 1.3541 Loss_G: 0.6636 NLL_Loss:-0.016512\n",
            "ÙØ²ÙÚ©Ø§Ù Ú©Ø¬Ø§ ÙØ¹ÙØª Ø¹Ø¯ÛÙ Ø¢ÙÚ©Ø³ ÚÛØ²Û ÙØ¹Ø§Ø¯Ø§ Ø®Ø³ØªÙ Ø­Ø´Ù ÙØ³Øª Ø¨Ø²Ù Ø§Ø­ÙØ§Ù ÙÙØªÙØ§ Ù¾Ø§Ú©ÛØ²Ù Ø¹Ø§Ø´ÙØ§ÙØª Ø¬ÙØ§Ø± Ø¨ÛØ´Ù Ø³ÙÚ¯Ø³Ø§Ø± Ø¯ÛØ¨Ø§ Ú¯Ø°Ø± Ø¯Ø§Ø±Ø§ Ø¯Ø§Ø±Ø¯ Ø§Ø«Ø± Ù¾Ø± ÙÛØ±Ø§ÙÛ ÛÚ©Ø³Ø± ÙÙØ§Û ÙØ°Øª ÙÙØ§ÙØ¯Û Ø³Ø§Ø±Ø¨Ø§ÙØ§ \n",
            "2\n",
            "[2/10][0/313] Loss_D: 1.3507 Loss_G: 0.6639 NLL_Loss:-0.016518\n",
            "Ø¯Ø§Ø±Ø¯ Ø§ÛÙÚ© ÙÙÛØ§ Ø®Ø¬Ø§ÙØª Ø¨Ø¯Ø§ÙÚ¯ÙÙÙ ØµØ¨Ø±Û Ø®Ø§ÙÙâØ¢Ø±Ø§ÛÛ Ø±Ø§Ø¯Û Ø®Ø¯Ù ÚÙÙÛÙ Ø¢Ø±Ø§Ø³ØªÙ ÙØ®ÙØ±Ø¯ÙâØ³Øª ÙØ³ØªÙÙØ¯Ø§Ù Ø§ÙØ³Ø§ÙÙâÛ ÙÙÙÙ ØªØ³Ú©ÛÙ Ù Ø¨Ø±ÙØªÛ Ù¾ÛØ±Ø§ÙÙ Ú¯Ø± Ø®ÙØ® ÙÙÙÙØ³Øª Ø¨Ø§Ø¯ ÙØ°Øª Ø¯Ø±Ø§Ø²Ø§ ÙÙÚ© ÙØ´ÙÙ Ø³Ø±Ù ÙØ§ÙÛ Ø¬Ø§ÙÙØ± \n",
            "[2/10][100/313] Loss_D: 1.3499 Loss_G: 0.6633 NLL_Loss:-0.016512\n",
            "ÙÙØ³Ø§ÛÙ Ø¬Ø³ÙÙØ§ Ú¯Ø°Ø§Ø´Øª Ø¨Ø± Ø¯Ø§Ø¯ÙØ¯ Ù¾Ø§Ú©ÛØ²Ù ØªØ±Û Ø¬ÙØ§ÙØ´ Ø¬ÙØ¨Ø´ÙØ§Û ÙØ­Ø§Ø¨Ø§ Ø­ÛÙØ§Ù Ù¾Ø§Ú©ÛØ²ÙâØ³Øª ÚØ´ÙÙ ÚÙÙØ§ÙâØ¨ÙØ¯ Ø¯ÛÙ Ø®Ø§Ø·Ø± Ø±Ø¨ ÙØ§ÙÙ Ø±ÙÚ¯âÚ©Ø±Ø¯ÛÙ Ù¾ÙØ¬Ù Ù¾Ø§ÛÙ Ú©Ø§ØºØ° ÙÙØ§ÙØ¯Ù Ø­ÙÙØ§ Ø¯ÛØ¨Ø§Û Ú¯ÙØ¬Û ÙÙÛâØ¢ÛØ¯ Ø¬Ø§ Ø´Ø§Ø¯ÛØ´ Ø§Ø±Ú©Ø§Ù \n",
            "[2/10][200/313] Loss_D: 1.3527 Loss_G: 0.6634 NLL_Loss:-0.016514\n",
            "Ø§Ø²ÛØ±Ø§ ÙÙØ³âÙØ§Û ÙÙØ±Ø­ Ø¨ÙÚ¯Ø± Ø³ÙÚ©ÙØ§Ø±Ø§Ù ÙØ¹Ù Ø¨Ø¯Ø±ÙØ¯ Ø¨ÛâÙÙØªÙØ§ ØªÙØ§ÙÚ¯Ø± ÙØ³ØªÙÙØ¯Ø§Ù Ø´ÙÙØ´Ø§ÙÛ ÙÙØ±ÙâÛ Ø¨ÛâØ§Ø®ØªØ± Ù¾Ø§Ú©ÛØ²Ù Ú©ÚÛ ÙØ§Ù ÙØ·ÙØ¨ ÙØ§Ù ÙØ§Ø² ÙÙÛØ´Ù Ø´ÙØ¯ ØªØ¯Ø¨ÛØ± Ú©Ø´Û Ù¾ÛØ±Ø§ÙÙ Ø§ÙØ³Ø±Ø¯Ú¯Û ØªÙÙÛÙ Ø§Ø¹ØªØ¯Ø§Ù Ø§ÙØ¯Ø±ÛÙ Ø¢ÙØ±Ø¯ Ú©ÚÚ© \n",
            "[2/10][300/313] Loss_D: 1.3496 Loss_G: 0.6633 NLL_Loss:-0.016514\n",
            "Ú¯ÙÙØ±ÙØ§ Ø§ÙØ¯ÙÙÚ¯ÛÙ ÙØ¯Ø­Ø´ Ú¯Ø±Ø¯Ø§ÙÙØ¯ Ú¯Ø°Ø§Ø´Øª ÙÙØ¬Ø§ ÙÛØ§Ø¨Ø¯ Ø³Ø± Ù¾Ø³ÙØ¯Ø¯ Ù¾Ø³ÙØ¯Ø¯ ÙÙÛØ¯Ø§ Ú©Ø±Ø¯Ù Ø·Ø±ÙÙ Ú©ÙÛ Ø³Ø§ÛÙ ÙÙÙ ÙØ¬ÙÛØ¯ ÙØªÙØ§Ù Ø¨Ø§Ø²ÛÚÙâÛ Ø¹Ø§Ø´ÙØ§Ù Ø¢Ø±Ø§ÛØ¯ Ø¢ÙÙØ² ÙÙÛØ´Ù ÙØ²Ù Ø¯Ù ÙØ¯Ù Ø²ÙØ¯ Ø¬ÙÚ¯ ÙØ¨Ù Ú¯ÙØªÙâÛ \n",
            "3\n",
            "[3/10][0/313] Loss_D: 1.3440 Loss_G: 0.6634 NLL_Loss:-0.016525\n",
            "ÙÙØªÙ Ø³ÛÙÛÙ ÙØ´Ú© ÚÙÙØ§ÙâØ¨ÙØ¯ ØªØ§Ø±Ú© Ø§Ø²ÛØ±Ø§Ú© ØªØ¨Ù ÙØ±Ø¯Ø§ ÙØ¹Ù ÙØ¨ÙâÛ Ø¬ÙØ­Ø±Û Ù¾ÛØ´ Ø¨Ù Ø¹ÙØ¯ ÙÛØ´ Ø±Ù ØªÙØ®ØªØ± ØµÙØ±Øª Ø§Ø¶Ø¯Ø§Ø¯ Ø¨ÛâØ§Ø®ØªØ± Ø¹ÙÙ Ø§Ø«Ø± ÙÛØ±Ø§Ù Ø§ÙÚ©ÙÛ Ù¾Ø§Ú©ÛØ²Ù Ø³Ø¹Û Ø¨Ø§Ø¨Ù Ø¨Ø³ÙÙ Ø±Ø³ÛØ¯ ØªÙØ§ÙÚ¯Ø± \n",
            "[3/10][100/313] Loss_D: 1.3444 Loss_G: 0.6635 NLL_Loss:-0.016515\n",
            "Ø¨Ø²Ù Ø±Ù Ø´ÙØ± ÙØ³Ø¬Ø¯âØ´Ø¯ÙâÚÙÙ ØºÙ Ø®Ø§Ø·Ø± Ø´Ø¨ÙØ§ ÛÚ©Ø±Ù Ø­ÛØ±ØªâØ·Ø±Ø§Ø²ÛØ³Øª Ú¯ÙØ´Ù ÚÙÙâØ¨ÙØ¯Û ÚÙÚ¯Ø§Ù Ø¹ÙØª ØªÛØ± Ø¬Ø§ ÙØ§Ø¯Ø± Ø®Ø¬Ø§ÙØª Ø¸Ø§ÙØ± Ø®ÙØ¨ Ø¨ÛØ¯Ù Ú¯ÙØ± Ø§ÙØ¯ÙÙ ÙÙØ¬Ø§ ÙÛÚ© Ø­Ø±Øµ ÙØ±Ø§Ù ÙÙÙÙ ÙØ´ Ø³Ú¯ ÙØªÙØ§ÙÛÙ \n",
            "[3/10][200/313] Loss_D: 1.3430 Loss_G: 0.6626 NLL_Loss:-0.016515\n",
            "Ø³Ø§Ø­Øª Ø¨Ø±Ø®Ø§Ø³ØªÙ ÙÙØ±âØ¨ÙØ¯Û ØµÙØ§Øª Ø®Ø§ÙÙ ÙØ¬Ø±ÙØ­ ÙÙØ§Ø± Ø°Ø§ØªØ´ ØµÙØ§ØªØ´ ÙÙØ´ÛÙ Ø¯ÙØ± ÙØ§ÙÙØ§Ø±Ù Ø¨ÛâØ®Ø±Ø¯Ø§Ù Ø¨Ø±ÛÙ Ø¬Ø§Ø¨ÙØ³Ø§ Ø¨Ø§Ø²Û Ø­Ø§Ù Ø¨Ø´ØªØ§Ø¨Û Ø¢Ø±Ø¯ Ø·Ø¨Ø¹ Ø²Ø¨Ø§ÙÙ Ø²Ø¨ÙØ±Ù Ø´Ú©Ù ÙØ´ÙØ¯ Ú¯Ø±Ø¬ÛØ¨ ÙØ¯Ø§ÙØ³Øª Ø³Ú©Ø± Ø¯ÙØ¯Ù Ø®Ø§ÙÙâØ¢Ø±Ø§ÛÛ ØªÙÚ©ÛÙ \n",
            "[3/10][300/313] Loss_D: 1.3419 Loss_G: 0.6632 NLL_Loss:-0.016520\n",
            "ÚÚ©ÙÙ ÙÙÛâØ¨Ú©Ø´Ø¯ ÙÛØ§Ø¨ÙØ¯ Ø²ÛØ¨Ø§ Ù¾ÛØ±ÙØ²Ù PAD ÙØ­ÙØª Ø¨ÛâÙÛØ§Ø² sep ÙÙØ´âØ¨ÙØ¯Û Ø´Ú© ÙÙØ§ÙØ§ Ø­Ø±Ø² ÙÙØªØ§Ø­ ÚÙ Ú©Ø±Ø¯ÙâØ§Ù Ø¯ÛØ¨Ø§ ÙØ±Ø§ ØµØ­Ø±Ø§ÙØ´ÛÙâØ§ÙØ¯ Ø°ÙÙÛ ÙÙÙØ¦ ÙÙØ§Û ÙÙÙ ÙÛØ¶Øª ÙØ±Ø¯ÙÛâØ¨ÙØ¯ Ø§ÙÙØ§ Ø±ÙÛØ´ ÙØ®Ø±Ø§Ù Ø³Ø¨Ø²Û ÙØ¹Ù \n",
            "4\n",
            "[4/10][0/313] Loss_D: 1.3365 Loss_G: 0.6633 NLL_Loss:-0.016519\n",
            "Ø®ÛØ§ÙØª Ø±ÙØ´Ù Ø¢Ø¯Ù ØªØ¹Ø±ÛÙ ÙÛÙØ²Ø¯ Ø´Ù Ø¨Ø´ØªØ§Ø¨Û Ø´ÙØ§Ø®Øª ÙÙØ±Ø¯ BOS ÛØ§ÙØªÙØ¯ Ø¨Ø´Ú©Ù ÙÙÙ Ø§Ø±Ú©Ø§ÙØ´ Ú©Ø´ØªÙ Ø¨Ø³ØªØ§Ù ÙØ¬Ø±ÙØ­ Ø¯Ø§ÙØ³ØªÙ Ø¢ÙØ§Øª ÙÛØ±ÙÚ¯âØ³Ø§Ø²Û Ù¾Ø±ÙØ± Ø±Ø§Û Ø¯ÛØ¯Ù ÙÚ¯Ø§Ø± ÛØ±Û ÙÙØ§Ø³Ø§ Ø³Ø§ÛÙ ÙÛ ØªØ¹ÙÛØ± Ø®ÙØ§Ø¨ \n",
            "[4/10][100/313] Loss_D: 1.3354 Loss_G: 0.6622 NLL_Loss:-0.016507\n",
            "Ø¯ÙØª Ø¨Ù ÙØ§ÙØ¯ÙâØ§Ù Ú¯ÙØª Ú©Ø±Ø¯Ú¯Ø§Ø±Øª Ø¢ØªØ´Û Ø­Ø´Ù ÚÙÚ¯Ø§Ù ÙÙØ§Ù Ø³ÙØ¯Ø§Û Ø¢ÛÛÙÙâÙØ§Ø±ÛÙ ÙØ§ØµØ±Ø§Øª ÙØ§ÙØ§ ØªÙØ²ÛÙ ÙÙØ¯ÛÙ Ø³Ø¹Û ØªÙ¾Ø¯ ÙÛØ§Ø¶ Ø§Ø´Ø¨Ø§Ø¹ Ø²ÙØ±ÙâÛ Ø§ÙØ³Ø±Ø¯Ú¯Û ØµØ§Ø¦Ø¨ Ø³Ø±Ø§Ø³Ø± Ú©Ø±Ø¯Ù Ú¯Ø´ØªÛÙ Ø¨ØªÛØº Ø¯ÛØ¨Ø§Û ÙÛØ±Ø§ÙÛ Ø³ÙØ± Ø¨ÛÙÙ \n",
            "[4/10][200/313] Loss_D: 1.3350 Loss_G: 0.6622 NLL_Loss:-0.016508\n",
            "ÙØ±Ø¯Ù ØµÙØ§ Ø§ÙØ¨Ø§Ù Ú¯Ø±ÙâØ¨Ø³Øª Ú¯Ø°Ø§Ø±ÛÙ ÙØ¯Ø­ ÙÙ ÙØ³ØªØ¹Ø¬ÙÙ ÙØ¬Ø± ÙØ±ÙØ¯ Ú©ÙÛ ÛÚ©Ø³Ø± Ø¨Ø¯ ÙÙØ§Ø¯Ù Ú¯Ø±ÙØªÙ ÙØ§Ù¾Ø³ÙØ¯Ù ÙØ³ÛÙÛ Ú¯ÛÙØ§Ù Ø·ÙØ§Ù ÙØµÙÙØ§Û Ø³ØªÛ Ø§ÙØ¬ÙØ§Ù Ø§ÙØ¯ÙÙÚ¯ÛÙ Ø´Ø¨ÙØ§ Ø¨ÛØ¯Ù ÚÛÙ Ø¯Ø§ÙØ¯ Ø³ÙÙÙ ÙØ±ÙØ§Ù Ø´Ø±ÛÙ \n",
            "[4/10][300/313] Loss_D: 1.3347 Loss_G: 0.6626 NLL_Loss:-0.016520\n",
            "Ú¯ÙØ¬Ø´ Ø¨Ú¯ÙØªÙ ÙØ§ÙØ³Øª ØºÙØ§ØµØ³Øª Ø±ÙÛ ÙÚ©Ø§ÙÛ Ø®Ø´Ù ØµØ¨Ø±Û Ø³Ø§Ø² Ú¯Ø±ÚÙ Ù¾Ø±ÙØ±Ø¯Ú¯Ø§Ø± Ø§ÙØ¯Ø± Ú©Ø§ÙØ¯Ø§Ø´Øª ÙØ§ÛÙ Ø²Ø¨Ø§ÙÛ ÙØ±ÙÛ Ø¨Ú¯Ø³Ø³Øª Ø³Ø§ÛÙ ÙØ§ÙÙØ§Ø±Ù ÙÚ¯Ø´Ø§ ÙØ§Ø¬Ø±Ø§ Ø§ÙØ¬ÙØ§Ù ÙØ¨ÙØ¯Û ÙØ¯Ø­ ÙÙÙ ÚÙØ¯Ø§ÙâÚ©Ù ÙØ¯Ù ÛØ§Ø³ Ø¨Ø§Ø´Ù Ø¨Ú©Û \n",
            "5\n",
            "[5/10][0/313] Loss_D: 1.3266 Loss_G: 0.6622 NLL_Loss:-0.016520\n",
            "ÙØ¬Ù ÙØ§Ø®Ù Ø¨ÛÙ ÚÙØ¯Û Ø®ÙØ¨ Ø±ÙÛØ§Ù Ú©ÙØ§Ù Ø§Ø² Ú©Ø§ÙÚ©ÙØ¯ Ø¯Ø§ÙÙ Ø±ÙÙÙØ§ÛØ§Ù Ø¯Ø±ÙÙÙ ÙØ­Ù Ø­Ø±Ú©Øª Ù¾Ø§Ú© Ø´Ø§ÙÙ ÙØµÙ Ú¯Ø°Ø± Ø§Ø´Ø§Ø±Øª ÙØ±Ù Ø§ØµØ·ÙØ§Ø¹ Ø¯ØºÙ ØµÙØ±ØªÛ Ø±ÙØ§ ØªØ¹ÙÛØ± ÙÚ¯Ø³ ÙØ¨ÛÙØ¯Ø´ Ø§ÙÙ ÚÙØ§Ù Ø¨Ø³Ø§Ù \n",
            "[5/10][100/313] Loss_D: 1.3283 Loss_G: 0.6624 NLL_Loss:-0.016514\n",
            "Ø°Ø§ØªØ´ ØµØ§Ø¦Ø¨ ÙØ¯Ø§Ù Ø²ÙÛ Ø§Ø´Ø§Ø±Øª ØªÙ Ø§Ø´Ú© Ú¯Ù ÙØ¨ Ø§Ø³Øª ÙØ¨Ø§Ø´ ÙÙÛÙÙØ§âØ¨ÙØ¯ Ø¯ÙÙ ÙØ§ØºØ³ØªâØ§Û ÙÚ¯ÛØ± Ø¨Ø§Ø² Ú¯ÛØªÛ Ø±Ø§Ø³Øª Ø¬Ø§Ù ÙØ¯ØºÙ ÙÙØ±Ø¯ ÚÙÙØ§Ù Ø¯ÛØ¨Ø§ Ø«ÙØ§Ú¯ÙØªÙ Ø³Ø§ÙÛØ§Ù ÙØ²ÙØ¯ Ø³Ø§ÙÙØ§ Ø§ÙØ´Ø§ÙØ¯Ø´ Ø¨Ù ØªÙØ§ÙÛ \n",
            "[5/10][200/313] Loss_D: 1.3289 Loss_G: 0.6621 NLL_Loss:-0.016518\n",
            "Ø±Ø¨ÙØ¯Û Ø¨Ø±ÙÙÙØ¯ Ø¯Ø§Ø±Ø§ ØªÙØ§ÙÛ Ø¯Ø³ØªÛâÚ©Ù Ø´Ø®Øµ Ø¨ÙÙØ§Û Ø¨ÛØ§Ø¨Ø¯ Ø¬Ø§ÙÙØ± Ø¨ÛØ±ÙÙ Ø¨Ù ÙØ±Ø­ÙØª ÙØ¬ÙÙÙ Ú¯ÙÛ ÛØ§Ø³ Ø¯ÛÙ ÙÙÛ ÚÙØ¯Ø§ÙâÚ©Ù Ø²Ø¨Ø§ÙÙ ÙÙÛâØ¢ÛØ¯ Ø¨Ú¯Ø³Ø³Øª ÙÙÚ©Ù Ø§ÙâØ¨ÙØ¯ÛÙ ØªÙÙØ§ Ø¨Ø§Ø±Û ØªÙØ§ÛÙâØ§Û Ø´ÛØ® ÛÚ©ØªØ§ ÙÙØªØ´ ØµØ­Ø¨Øª \n",
            "[5/10][300/313] Loss_D: 1.3268 Loss_G: 0.6620 NLL_Loss:-0.016518\n",
            "Ø´ÙÛ ÙØµØ± Ø§Ø¬Ø±Ø§Ù Ø¨Ø¯Ø³Ú¯Ø§ÙØ§Ù ÙÛÚ© Ù¾Ø±ÙØ± Ø¸Ø§ÙÙ ØªÙØ­ÛØ¯ Ù¾ÛÚ©Ø§Ù Ø­Ù ÙØ§ÙØ§ ÙØªØ¹Ø§ÙØ¨âØ¨ÙØ¯ ÙÙØ³âÙØ§Û ÙØ¹Ù ÙØ§Ø² Ø¯Ø§Ù ÙØµÙÙØ§Û ÙÚ¯ÙØ¨Ø§Ù Ø¬ÙÚ¯ Ø³Ø±Ø¨ÙÙØ¯Û Ø²ÙÙØ§Ø± Ø¢Ø±Ø§ÛØ¯ ÚÙØ§ÙÚÙÙ ÙÙÛÚ©Ù Ú¯ÙØªÙ Ø¨Ø±Ú¯âØªØ± Ø¯Ø±ÙÛØ´ Ø¯Ø±ÙØ§ÙØ´ ÙÛâØ±ÙØ´Ù Ú©Ø³Ù \n",
            "6\n",
            "[6/10][0/313] Loss_D: 1.3219 Loss_G: 0.6622 NLL_Loss:-0.016515\n",
            "Ø¨Ø§Ø·Ù Ø®ÙØ±Ø´ÛØ¯ Ø¨ÚÚ¯Ø§Ù ÛØ±Û ÙØ§ÙÙÙØ¯ÙØ¯ Ø¹Ø¨Ø±Øª Ø²Ø­ÙØªâØ¨ÙØ¯ Ø¯Ø´ÙÙ Ø°Ø§Ú©Ø± ÙØªÙØ§Ù ÙØ±ØªÙØªÛ Ø¸Ø§ÙÙ ÙÙØ§Û Ø±ÙÛØ§Ù Ù¾ÛØ±Ø§ÙÙ ÚÙÚ¯Ø§Ù ØªØ´Ø¨ÛÙ Ø§ÙÙØ§Ù Ø®ÙØ¯ÛØ¯ ÙÙÙØ± Ø¯Ø±ÙØ§Û Ø³ÙØ®ØªÙ ÙÙØªØ± ÙØ³ØªØ¹Ø¬ÙÙ Ø­ÙØ§ Ø¬ÙØ­Ø±Û ÙØ®ÙØ§ÙØ¯ Ø¨ÙÛØ§Ø¯ ÚØ´Ù ØªØ¹Ø§ÙÛ \n",
            "[6/10][100/313] Loss_D: 1.3228 Loss_G: 0.6619 NLL_Loss:-0.016511\n",
            "Ø²ÙØ¯ ÚÙÚ¯Ø§Ù ÙØ¬ÙØ³ Ø«ÙØ§Ú¯ÙØªÙ Ø¬Ø¨ÛÙ ÙÚ©Ø§ÙØ§Øª Ú¯Ø±Ù ÙØªÙØ­ Ú©Ø§ÙØ± ÙØ®Ø± Ø¨ÙÚ¯Ø± ÙØ´ÙØ¯ Ø¯Ø´ÙÙ ÙÙÙ Ø·Ø§ÙØª ØªÙØ¬ÛØ¯ Ø¢ÙØ§Øª Ø±ÙØªØ§Ø± Ø´ÙÛ Ø²ÙØ¯Ù Ø¬ÙØ¯Ø´ ÙØ¨Ù Ø±Ø³ØªÙ Ø³ÙØ®ØªÙ Ø¯ÙØ³ØªâØ¨ÙØ¯ ÙÚ©Ø±Øª Ø¢Ø±Ø§ÛØ¯ ØªÙÚ¯ Ù¾Ø§Ø´Û Ù¾ÛÙÚ¯ÙÙ \n",
            "[6/10][200/313] Loss_D: 1.3215 Loss_G: 0.6620 NLL_Loss:-0.016518\n",
            "Ø¸Ø§ÙÙ Ø¨Ø±Ø¬Ø³Øª ÙØ¨Ø§Ø´ Ø´Ù Â» Ø²ÙØ¯ ÙØ± Ø¨Ø±ÙÙ Ø§ÙÙØ§ ÙÛØ§Ø²Ø§Ø± Ø°Ø±ÙÙ ÙØ±ÙØ§Ù Ø¨Ø®Ø´Ø´ Ø¯ÛØ¯ ÙØ§ÙØ¯ Ú©ÙÙ ÙÚ¯Ø±Ø¯ÛâÚ¯Ø± Ú©Ø±ÛØ¬Û Ø¨ÛâÙÛÙØªÙØ¯ Ø±Ù Ø²Ø¯ÛÙ ÙÛÙØª Ú©Ø§ÙÚ©ÙØ¯ Ø¢ÛÛÙÙâÛ Ø¢ÙÚ¯Ø§Ù Ø³Ø§Ù Ø²Ø­ÙØªâØ¨ÙØ¯ Ø³ÛÙÙâÙØ§ ØªØ±Û ÙÛØ§Ø±Ø¯ \n",
            "[6/10][300/313] Loss_D: 1.3197 Loss_G: 0.6612 NLL_Loss:-0.016513\n",
            "Ú©Ù Ø®ÙØ§Ø¨ ÙÚ©Ù ÙØ§ÙÙ Ø§ÙÚ©ÙÛ Ø®Ø¯Ù Ø²Ø´ÙÙ Ø¹Ø² Ø¨ÛØ±ÙÙ Ø¯Ø§ÙÙ Ø¨Ø§Ø¯Ø§Ù Ø¯Ø±Ù Ø¬Ø§Ø±Û ÙØ·ÙØ¨ ÙÚ©Ù Ú¯Ø±Ø¬ÛØ¨ ÙÚ©ÙÛ Ø¯ÙØ¨Ø± ÙØ³Ø§ÙØ§ ÙØ´Ú©Ù Ø¯Ø§Ù Ú¯ÙØª Ú¯ÙØ§ÙÛ Ø´Ø¨ÙØ§ Ø¯Ø±Ø§Ø²Ø§ Ø¨Ø¯Ø§ÙØ¯ Ú¯Ø±ÙØ§ Ø¨Ø¯Ø§ÙÚ¯ÙÙÙ ÙÙÛØª Ú¯Ø±Ø¯Ø§Ù \n",
            "7\n",
            "[7/10][0/313] Loss_D: 1.3159 Loss_G: 0.6616 NLL_Loss:-0.016512\n",
            "Ø´Ø¨ Ø§Ø¸ÙØ§Ø± ÙÙØ±Û Ø·ÙÙÛ Ø´Ú¯Ø±Ù ØªÙÙÛÙ Ù¾ÙÙØ§ Ø¨ÙÙØ§Û Ø´Ø§ÛØ¯ ÙØ²Ø§Ø± Ú©Ø¬Ø§ÛÛ Ú©ÚÛ Ø¨Ø³ ÙØ±ÚÙØ¯ Ø¹ÙÙ Ú©Ø§Ù Ù¾Ø¯ÛØ¯Ø§Ø± Ø­ÛÙØ§Ù Ø´ÙØ¯Ø§ Ø¢ÙØ±Ø¯ÙâØ³Øª Ú©Ø¬Ø§ÙØ³ØªØ´ Ø¹Ø§Ø±Û ÙÛâØ±Ø§ Ø´Ø±ÛÙ Ø¯ÙÙ Ø²ÙâØ§Û Ú©Ø§ÙØ¯Ø§Ø´Øª ÙØ¯Ø­ Ú©Ø±Ø¯ÙâØ³ØªÛ ÙØ¯Ø±ØªØ´ \n",
            "[7/10][100/313] Loss_D: 1.3152 Loss_G: 0.6614 NLL_Loss:-0.016511\n",
            "Ø®Ø´Ù Ø­ÙØ¯ ÙÙØ³âÙØ§Û Ø§Ø¨Ø±Û Ø°ÙÙÛ ÙØ¯Ø³Ø§ ÙØ®ÙØ±Ø¯ Ø¢Ø¨Ú¯ÙÙ ØªØ´ÙÙ Ø³Ø±Ù Ø«Ù Ø±ÙØ´Ù Ø²Ø§Ø± ÙÛØ±Û Ø­ÛÙ Ø®Ø¶Ø±Ø§ Ú©Ø±Ø¯Ú¯Ø§Ø±Øª Ø§ÙÙ Ø¯Ø±Ø¢ÙØ¯ Ø¢Ø¯Ù ÙØ´Ø±Ù Ø§ÙÙØ§Ù Ø±ÙÛØ¨Øª ÙÙÙ ÙØ´ÙØ¯ ØµÙØ¹ Ú¯ÙÙÙ ÙÛÚ©ÙØ§Ù Ø¨ÛØ´Û ÙÙØ±âØ¨ÙØ¯Û \n",
            "[7/10][200/313] Loss_D: 1.3139 Loss_G: 0.6615 NLL_Loss:-0.016516\n",
            "Ú©Ø§Û Ø¯ÙØ³Øª ÙØ§Ø¯Ø± ØµÙØ§ØªØ´ ÙÙ Ø¨Ú¯Ø´Ø§Û Ø¯Ø±ÙÙÙ Ø³Ù¾Ø§ÙØ´ Ø­Û ØµÙØ¨Ø§ Ù¾ÛØ¯Ø§Ú©Ø¬Ø§Ø³Øª Ø¬Ø§ÙÙ ØµØ§Ø¦Ø¨ Ø¨Ø§Ø±Û Ø¯ÛØ¯ ÙÙØ§ ÙØ¯ÙÙØ´ ÙØºØ¨Ø± Ø¬Ø³ÙÙØ§ Ø´ÛØ¯Ø§ ÙØ³Ø§Ø¯ Ú¯Ø±Ø§ÙÙØ§ÛÙ ÙÙØ§Ø¯ÛØ±Ø³Øª Ø¨ÛâØ§Ø®ØªØ± Ø¯ÙØ¨Ø± Ù¾Ø§ÙØ§ÛØ¯Ø´ Ø±Ø¹ÛØª Ø³ÙØ¨ Ø¨Ø´ØªØ§Ø¨Ø¯ ÙÙØ¯Ø§Ø± \n",
            "[7/10][300/313] Loss_D: 1.3140 Loss_G: 0.6617 NLL_Loss:-0.016519\n",
            "Ø¨ÙÙØ¯Û Ø¨Ø§ÙÛ Ú¯Ù Ø®ÙØ§ÙØ¯ ÙØ¯Ø§ÙÛ ÛÚ©Û Ø¨Ø§ÙÛ ÙÛØ§Ø¶ ÙØ¯Ø§ÙØ¯ Ø¯Ø¹ÙÛ Ø³Ù¾Ø§Ø³ ØªÙ¾Ø¯ ÙÙØ´âØ¨ÙØ¯Û ØªØ­ÙÛÙ ÙÙØª ÙÛØ±Û ÙØ¨ÙâÛ ÙØ®ÙØµØ§Ù ÙØ§ÙÙÙØ¯ÙØ¯ ÙâØ¨ÙØ¯ Ú¯ÙÛØ¯ Ø®ÛØ§ÙØª Ø²ÙØ§ÙÛ ÙÙØªÙØ§ ÙØ§ÙØ±Ø§Û Ø¨ØªÙØ§ÙØ¯ Ø¨ÛØ§Ø¨Ø§Ù ÙØ§Ú¯ÙØ§Ù Ú©ÛØ³Øª ØªÙØ§ÙØ³Øª \n",
            "8\n",
            "[8/10][0/313] Loss_D: 1.3091 Loss_G: 0.6615 NLL_Loss:-0.016512\n",
            "ÙØ§Ø¨ÛÙØ§ ÙÙØ±âØ¨ÙØ¯Û ÙØ®ØªÚ©ÛÙÛÙ Ø¨Ø§Ø´Ø¯ Ø§ÙØªØ§Ø¯ÛÙ ØªØ®ØªØ´ ÙÙÛ Ú¯Ø±Ø¯ÛÙØ§Û Ø­ÛÙ ÙÙØ§ÙØ¯Ù Ø¯Ø§Øº ÙÛØ¹Ø§Ø¯ ÙØ²ÙØ¯ ÙØ§ÙØ§ Ø¬ÙØ§ÙØ´ ØªØ´Ø¯ÛØ¯Ø´Ø§Ù Ø¨ÙØ±Ù Ø¹Ø±ÙØ³Ø§Ù Ø§ÙÛØ±Ø§ ÚÙ Ø«ÙØ§Û Ø³ÙÙØ§ÙâØ²Ø¯Ù ÛØ§Ø±Û Ø¨ÛâÚ¯ÙÙ Ø³ÙØ§Ø¯Ø´ ÙÙØªÙØ§ Ø¢ÛÙÙâÛ ÙØ±Ø§ Ø®ÙØ±Ø¯ ÙÙØ§ÙØ§ \n",
            "[8/10][100/313] Loss_D: 1.3076 Loss_G: 0.6612 NLL_Loss:-0.016521\n",
            "ÙØ§ Ø¨Ú©Û Ú¯ÙØ´ Ø³ÙÛ ÚÙØ¯Ø§Ù Ø­Ø¯ Ø¹Ø§ÙÙÛ ÙÙÙ Ø´Ø§Ø¯Ú©Ø§ÙÛ ÙØ¯ÙÙØ´ ØªØ±Ø³Ø§Ù Ø®ÙØ¨Û ÙÛÙÛ Ú©Ø§Ù Ú©Ø§ÙØ±Ù Ø¢Ø¨ Ø¨Ù¾ÛÙØ³ØªÙâØ³Øª ÙÛ Ø¬ÙØ¯ Ø±ÙØ­ Ø¢ÙØ±Ø¯ Ø¬Ø§Ø¨ÙØ³Ø§ Ø¢Ø±Ø¯ Ø®ÙÛÛ ÙØ³Ø¬Ø¯âØ´Ø¯ÙâÚÙÙ Ø¨ÛØ¯Ø§Ø± Ø¨Ù¾ÛØ± ÙØ¨Ø§Ø´ Ø§ÙØ±ÙØ² Ø¨ÛØ§ÙÙØ² \n",
            "[8/10][200/313] Loss_D: 1.3070 Loss_G: 0.6618 NLL_Loss:-0.016518\n",
            "Ø¯ÙØ¨Ø± Ø§Ø±Ú©Ø§ÙØ´ Ú¯Ø±Ø¯ÙÙ ÙØ°Øª ÙÙâØ§Û Ú¯ÙØªÙ Ù Ø¯Ø±ÙÙÙ ÙÙØ§Ø± ÙØ§ÙÙâÙØ§Û Ø§ÛÙ Ø­Ø§Ù Ø¨ÛâÙÛØ§Ø²Û Ø±Ø§Ø³ØªÛ Ø¢Ø³ÙØ§Ù Ú©ÙØ§Ù ØªÙÙØ§ ÙØ´ÙØ¯âØ§Û Ù¾ÛØ´ÛÙÚ¯Ø§Ù PAD ØµØ­Ø±Ø§ Ø¨ÛÙÙ Ø¯Û ÙØ§Ù ÙØ± Ø³Ø§Ø¹Øª Ø¬ÙØ§Ù ÙØ¬Ù Ø®ÙØ§Ø± Ú©ÛÙâØ´Ø¯Ù \n",
            "[8/10][300/313] Loss_D: 1.3057 Loss_G: 0.6612 NLL_Loss:-0.016521\n",
            "Ø¹Ø§Ø¯Ù Ø´Ø¨ÙØ§ Ø³Ø§Ø­Ù Ø³Ø±Ø§ÛØ¯ ÙØ³Ø¬Ø¯ Ø§Ø¹Ø¶Ø§ ÙØªØ­ Ø¨Ú¯Ø±Ø¯Ø§ÙÛØ¯ Ø±Ø§Ø¯Û Ø¢ÙØ§Ø²Ù ÙÛÙØ±ÙØ² ØªÙØ§ÙÛ Ø´Ø¨ÙØ§ Ú¯Ø´ØªÛÙ Ø²ÙØ¯ Ø¨ÙØ¯ÙâØ³Øª Ø·Ø§ÙØª Ø´Ø§Ø¯Û ÛÙÛÙ Ø¬ÙÙØ±Û Ø¨ÛâØ±ÙØ²Ù Ø¨Ø§Ø´ Ú¯ÙØ´ÙâÛ Ø±ÙÚ¯ Ú©Ø±Ø¯Ø§Ø± ØºØ§ÛØ¨ÙØ¯ Ø¯Ø±Ø§Ø²Ø§ ÛØ¯Ø§ÙÙÙ Ø¨Ø§Ø¯Ø§Ù ØµØ§Ø­Ø¨ \n",
            "9\n",
            "[9/10][0/313] Loss_D: 1.3029 Loss_G: 0.6614 NLL_Loss:-0.016516\n",
            "Ø®ÙØ±Ø¯ ÙØ²ÙÚ©Ø§Ù Ú¯Ø±Ø¯Ø¨Ø§Ø¯ Ø¯Ø§ÙØ³Øª Ø¢Ø³ÙØ§Ù ÛØºÙØ§ Ú©Ø§Ù Ú¯Ø¯Ø§ ÙÙÙØ§Ø±Ù ÛÚ©Ø³Ø± ÙØ±ØºØ²Ø§Ø± ÙÙÙ Ø§ÙØ³Ø±Ø¯Ú¯Û ÙØ§Ø®Ù ÙÙÙØ± Ø§ÙØ¯Ù Ø¯ÙØ± Ú¯ÙØ´ Ø³ÙØ¬Ø§Ø¨ Ø·ÙØ§Ù ÙØ­ÛØ· Ø´Ø§Ù Ø®Ø´Ú© Ø¨ÛØ¯ÙØ§Ù Ø¢Ø¯ÙÛ Ø§ÛÙâØ¨ÙØ¯ Ø¬Ø§Ø¨ÙØ³Ø§ ØµØ§ÙØ¹ Ø¹Ø±Ø¶ ÙÛÙÛ \n",
            "[9/10][100/313] Loss_D: 1.2995 Loss_G: 0.6614 NLL_Loss:-0.016518\n",
            "Ø®Ø§ÙÙØ§ÙÙØ§ ÙØ³Øª ÙØ¯Ø±Øª Ø¯ÛÙØ§Ø± Ø¯Ø±ÛØ§ Ø±ÙÛØ´ Ø¯Ø³ØªÙØ²Ø¯ ØªÙ Ø¨Ø¯Ø§ÙØ¯ Ø¨Ø§Ø¯ Ø¯ÛØ¯ÙâÛ Ø¨ÙØ§ÛÛ ÙÙÚ©ÙØª ÙÛØ±Û Ø®ÙØ§ÙÙØ¯ Ø¨Ø±Ø§Ø¨Ø± Ø­Ù Ø¨Ø±Ø§Ø¨Ø± ÙÙÛÙÙØ§âØ¨ÙØ¯ Ø­Ø±Ù Ú©Ø´Û ÙØ§ Ø±Ø³ØªÙ ØªØ¹ÙÛØ± ÙØ§ÙØ¯ ÚÙÙâØ¨ÙØ¯Û Ø¯Ø§Ø±Ø§ Ø¨Ø§Ø´Ù Ú©ÙØ§Ù ØªØ§ÙØª \n",
            "[9/10][200/313] Loss_D: 1.3000 Loss_G: 0.6617 NLL_Loss:-0.016510\n",
            "Ù¾Ø§Ø¯Ø´Ø§ Ø®ÛØ±Ù ØµØ¯ ÚÙØ¯Û Ø¨ÙØ¯ÙâØ³Øª ÚÛÙÛ Ù Ø¨ÛâØ¨ÙØ§ ÙØ³Ø§ÙØ§ Ø­ÛÙØ§Ù Ø³ÙÚ¯ ÙØ±Ø¯Ù Ø¯Ø±Ø§Ø²Ø§ ÙÛâÚ©ÙØ¯ ÙÙØ¸Û Ø¨Ø±ÙÙÙØ¯ ÙÙÛâØ±ÙØª ØªØ¨Ø§ÙÛ ÙØ§Ù ÙÛØ² ÙÙÚ© Ù¾Ø³ÙØ¯Ø¯ ÙØ§ÙÙÙØ¯ÙØ¯ ÙØ³Øª ÙÙ Ø¨Ø²Ù Ø¨Ø±ÙÙÛ ØªØ§Ø¨Ø³ØªØ§Ù Ø¯ÛØ¯Ù Ø²ÛØ¨Ø§Ø³Øª \n",
            "[9/10][300/313] Loss_D: 1.2970 Loss_G: 0.6616 NLL_Loss:-0.016510\n",
            "Ú¯Ù Ú©Ø³Ù ÙÙÙØ§ Ø·ÙØ§Ù Ø«Ù Ø±Ø§Ø¯ Ø«ÙØ§ ØµÙØ¨Ø§ ÙÙØ§ÛØ§ØªØ´ Ø³Ø¹Û ÙØ®Ø³ØªÛÙ Ø¯Ø±ÙØ§ÙØ´ ÙØ¯Ø§Ù Ø¯Ù ØºØ±Ø¶ Ø®Ø±ÙØ§ ÙÙØ§Ø²Û Ø§ÙÙØ§ ÙØ§Ø§ÛÙÙ ÙØ¹Ø§ÙÛ Ø®Ø§ÙÙ Ø³ØªØ§Ø±Ù Ú¯ÙØªÙ Ú¯Ø±ÙâØ¨Ø³Øª Ø¢Ø¨ ÚÙØ¯ÛÙ ÙØ§ÙÙ Ø±ÙÙ Ø­Ø§ÙÛØ§âÚ¯Ø± Ø¯Ø§ÙØ³Øª \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.randint(3, 5, (1,))"
      ],
      "metadata": {
        "id": "MQN0Grfz3tvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4614b2-4d5d-4328-d402-1b497f8310e3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.grad"
      ],
      "metadata": {
        "id": "x2XvYLyh4uo1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "632f5094-ca92-4477-bff4-a4200d03668b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-d40920619642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poems = pd.read_csv(\"/content/drive/MyDrive/NLU_Project/Data/test.csv\")\n",
        "# print(poems['poem_id'])\n",
        "i = 3\n",
        "poems_sequence =[]\n",
        "while True:\n",
        "            poem_i = poems[poems['poem_id'] == i]\n",
        "            # print(poem_i)\n",
        "            index_i = poems.index[poems['poem_id'] == i]\n",
        "            current_poem = \"\"\n",
        "            for p in index_i:\n",
        "                v_position = poem_i.loc[p,\"v_position\"]\n",
        "                verse =  poem_i.loc[p,\"poem_text\"]\n",
        "                # print(verse)\n",
        "                current_poem += verse \n",
        "                if v_position == 0:\n",
        "                    current_poem += \" \\t\"\n",
        "                if v_position == 1:\n",
        "                    current_poem += \" \\n\"\n",
        "            if len(current_poem)>0:\n",
        "              poems_sequence.append(current_poem)\n",
        "            i += 1\n",
        "            if i>596 :\n",
        "              break\n"
      ],
      "metadata": {
        "id": "NK5ZuS8HVPmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_h, state_c = netG.init_state(1)\n",
        "fake= netG(torch.tensor([[1]]), (state_h, state_c))"
      ],
      "metadata": {
        "id": "0f_nxAh2UOpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
        "for o in fake:\n",
        "        str1 = \"\"\n",
        "        for w in o:  \n",
        "          # print(w)     \n",
        "          str1 += d_load.vocab[w]\n",
        "          str1 +=\" \"\n",
        "          score = 0\n",
        "          poem = ''\n",
        "        print(str1)\n",
        "        for poems in poems_sequence:\n",
        "          s =  sentence_bleu(poems.split(\" \"),str1.split(\" \"))\n",
        "          if score < s:\n",
        "           score = s\n",
        "           poem = poems\n",
        "        print(score)\n",
        "        print(poem)\n"
      ],
      "metadata": {
        "id": "BC6DZZCJUuuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZiVq8Ywiy-bY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}