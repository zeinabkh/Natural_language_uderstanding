{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU_project_12_2_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install parsivar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2tuw9tIacyF",
        "outputId": "e21c158f-725f-49dd-a421-c0e6544b772a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: parsivar in /usr/local/lib/python3.7/dist-packages (0.2.3)\n",
            "Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/dist-packages (from parsivar) (3.4.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->parsivar) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.randint(25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "083VqaoT_IOf",
        "outputId": "703c5803-07c1-4703-fcdc-612f56642f69"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6qTYlz6M1pbN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from parsivar import Tokenizer,FindStems,Normalizer\n",
        "import os \n",
        "import re \n",
        "import torch \n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEtkQm9TyUYF",
        "outputId": "e808dc78-9e94-4d83-bbf5-ee52fa071020"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "  def __init__(self,batch_size,hidden_dim,vocab_l,window):\n",
        "    self.batch_size = batch_size\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.vocab_l = vocab_l\n",
        "    self.window = window\n",
        "    self.dis_init = \"uniform\"\n",
        "    self.gen_init =\"uniform\"\n",
        "    if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
        "        self.gpu = int(True)\n",
        "    else:\n",
        "        self.gpu = int(False)\n"
      ],
      "metadata": {
        "id": "SwI_5vLfT6aF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data_load:\n",
        "  def __init__(self,max_length, step):\n",
        "    # self.train_path = train_path\n",
        "    # self.test_path = test_path\n",
        "    # self.test_sequence = self.read_data(test_path)\n",
        "    self.vocab = []\n",
        "    # self.voocab_l = len(self.vocab)\n",
        "    self.max_length = max_length\n",
        "    self.step = step\n",
        "    self.load_dictionary()\n",
        "\n",
        "  def load_dictionary(self):\n",
        "    words = open(\"/content/drive/MyDrive//NLU_Project/vocab.txt\").read().split(\"\\n\")  \n",
        "    for w in words:\n",
        "      self.vocab.append(w)\n",
        "    # self.vocab.append(\"\\n\")    \n",
        "    # self.vocab.append(\"\\t\")  \n",
        "    self.vocab.append(\"eos\")\n",
        "    self.vocab.append(\"sep\")\n",
        "    self.vocab.append(\"PAD\")\n",
        "    self.vocab.append(\"UNK\")  \n",
        "    self.vocab.append(\"BOS\")  \n",
        "    self.vocab = sorted(self.vocab)\n",
        "    self.dict_word2id = { word :i   for i,word in enumerate(self.vocab)}\n",
        "    self.dict_id2word = { i : word  for i,word in enumerate(self.vocab)}\n",
        "    # self.dict_word2id ['PAD'] = 0\n",
        "    # self.dict_word2id [\"UNK\"] = 2\n",
        "    # self.dict_word2id [\"BOS\"] = 1\n",
        "    # self.dict_word2id [\"sep\"] = 3\n",
        "    # self.dict_word2id [\"eos\"] = 4\n",
        "    # self.dict_id2word[0]=\"PAD\"\n",
        "    # self.dict_id2word[2] =\"UNK\"\n",
        "    # self.dict_id2word[1] =\"BOS\"\n",
        "    # self.dict_id2word[3] =\"sep\"\n",
        "    # self.dict_id2word[4] =\"eos\"\n",
        "    self.vocab_l = len(self.vocab)\n",
        "    \n",
        "  def create_sequence_data(self,paths):\n",
        "    poems =  self.read_data(paths)\n",
        "    all_poem_seq = []\n",
        "    for poem_seq in poems:\n",
        "        word2id_sequence = []\n",
        "        words_list = poem_seq.split(\" \")\n",
        "        # print(words_list)\n",
        "        for word in words_list:\n",
        "\n",
        "          try:\n",
        "              word2id_sequence.append(self.dict_word2id[word])\n",
        "          except KeyError:\n",
        "              word2id_sequence.append(self.dict_word2id[\"UNK\"])\n",
        "        all_poem_seq.append(word2id_sequence)\n",
        "    X_gen = []\n",
        "    X_dis = []\n",
        "    Y = []\n",
        "    for poem_seq in all_poem_seq:\n",
        "      for i in range(self.max_length,len(poem_seq),self.step):\n",
        "         X_gen.append([1]+poem_seq[i - self.max_length: i-1])\n",
        "         X_dis.append(poem_seq[i - self.max_length: i])\n",
        "         Y.append(poem_seq[i - self.max_length+1: i+1])\n",
        "    return X_gen, X_dis, Y\n",
        "            \n",
        "  def read_data(self,paths):\n",
        "    poems_sequence = []\n",
        "    poems = pd.read_csv(paths[0])\n",
        "    i = 1\n",
        "    while True:\n",
        "            poem_i = poems[poems['poem_id'] == i]\n",
        "            # print(poem_i)\n",
        "            index_i = poems.index[poems['poem_id'] == i]\n",
        "            current_poem = \"\"\n",
        "            for p in index_i:\n",
        "                v_position = poem_i.loc[p,\"v_position\"]\n",
        "                verse =  poem_i.loc[p,\"poem_text\"]\n",
        "                # print(verse)\n",
        "                current_poem += verse \n",
        "                if v_position == 0:\n",
        "                    current_poem += \" sep\"\n",
        "                if v_position == 1:\n",
        "                    current_poem += \" eos\"\n",
        "            if len(current_poem)>0:\n",
        "              poems_sequence.append(current_poem)\n",
        "            i += 1\n",
        "            if i>599 :\n",
        "              break\n",
        "    for path in paths[1:]:\n",
        "        \n",
        "        poems = pd.read_csv(path)\n",
        "        # print(poems['poem_id'])\n",
        "        i = 1\n",
        "        while True:\n",
        "            poem_i = poems[poems['poem_id'] == i]\n",
        "          \n",
        "            index_i = poems.index[poems['poem_id'] == i]\n",
        "            current_poem = \"\"\n",
        "            for p in index_i:\n",
        "                v_position = poem_i.loc[p,\"v_position\"]\n",
        "                verse =  poem_i.loc[p,\"poem_text\"]\n",
        "                current_poem += verse \n",
        "                if v_position == 0:\n",
        "                    current_poem += \" sep\"\n",
        "                if v_position == 1:\n",
        "                    current_poem += \" eos\"\n",
        "            poems_sequence.append(current_poem)\n",
        "            i += 1\n",
        "            if len(poem_i)<1 :\n",
        "              break\n",
        "    return poems_sequence"
      ],
      "metadata": {
        "id": "HT1d0slv5I6U"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import os\n",
        "import pandas as pd\n",
        "paths = os.listdir(\"/content/drive/MyDrive/NLU_Project/Data/poems\")\n",
        "paths_arr = [\"/content/drive/MyDrive/NLU_Project/Data/poems/\" + f for f in paths]\n",
        "d_load = Data_load(30,1)\n",
        "# poems_list = d_load.read_data([\"/content/drive/MyDrive/NLU_Project/Data/train.csv\"]+paths_arr)\n",
        "# poems_list = d_load.read_data([\"/content/drive/MyDrive/NLU_Project/Data/train.csv\"])\n",
        "args = Args(32,40,d_load.vocab_l,d_load.max_length)"
      ],
      "metadata": {
        "id": "_JwbOgeD5scB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dff8556-d3a8-4d51-ca99-6e8a7e8d0038"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 745 µs, sys: 3.03 ms, total: 3.78 ms\n",
            "Wall time: 9.81 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(d_load.dict_id2word.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXj_Y1IS7Dkx",
        "outputId": "52e3e9ff-dd6b-4e75-88ee-1b1d33e37b7b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1817"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # from parsivar import Normalizer\n",
        "# import re\n",
        "# from parsivar import Tokenizer\n",
        "# my_normalizer = Normalizer()\n",
        "# my_tokenizer = Tokenizer()\n",
        "# words = {}\n",
        "# for poems in poems_list:\n",
        "#   poems = re.sub(\"[0-9a-z]|\\!|\\?|\\.|\\،\",\"\",poems)\n",
        "#   tokens = my_tokenizer.tokenize_words(my_normalizer.normalize(poems))\n",
        "#   for token in tokens:\n",
        "#     try:\n",
        "#        words[token] +=1\n",
        "#     except KeyError:\n",
        "#       words[token] =1"
      ],
      "metadata": {
        "id": "QWmMHkClPmvB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f = open(\"/content/drive/MyDrive/NLU_Project/vocab_text.txt\",'w')\n",
        "# for w in list(words.keys()):\n",
        "#     f.write(w)\n",
        "#     f.write(\"\\n\")\n",
        "# f.close()"
      ],
      "metadata": {
        "id": "sSge6ZhKUi-F"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_gen,X_dis,Y= d_load.create_sequence_data( [\n",
        "                                             \"/content/drive/MyDrive/NLU_Project/Data/train.csv\",\n",
        "                                             \"/content/drive/MyDrive/NLU_Project/Data/poems/سعدی.csv\"])\n",
        "len(X_gen)"
      ],
      "metadata": {
        "id": "nC9DQ6zetGis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d77b3ae7-2f19-4bce-8388-f4e6d0ca8f44"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "206162"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "dataset_gen = TensorDataset(torch.tensor(X_gen[:10000]),torch.tensor(Y[:10000]))\n",
        "dataloader_gen = DataLoader(dataset_gen, batch_size=args.batch_size)\n",
        "dataset_dis = TensorDataset(torch.tensor(X_dis[:10000]),torch.tensor(Y[:10000]))\n",
        "dataloader_dis = DataLoader(dataset_dis, batch_size=args.batch_size)"
      ],
      "metadata": {
        "id": "rqX68TNNA9sf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f = open(\"/content/drive/MyDrive/NLU_Project/vocab_text.txt\",\"w\")\n",
        "# for i in d_load.vocab:\n",
        "#   f.write(i)\n",
        "#   f.write(\"\\n\")\n",
        "# f.close()"
      ],
      "metadata": {
        "id": "HBQ8NNjf5qzl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        " \n",
        "\n",
        "class Metrics:\n",
        "    __metaclass__ = abc.ABCMeta\n",
        "    def __init__(self, name='Metric'):\n",
        "        self.name = name\n",
        "\n",
        "    def get_name(self):\n",
        "        return self.name\n",
        "\n",
        "    def set_name(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def get_score(self):\n",
        "        pass\n",
        "\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "class NLL(Metrics):\n",
        "    def __init__(self, name, model, loader, gpu=True):\n",
        "        super(NLL, self).__init__(name)\n",
        "\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.gpu = gpu\n",
        "        self.need_reset = True\n",
        "\n",
        "    def get_score(self, model=None, loader=None, ignore=False):\n",
        "        \"\"\"note that NLL score need the updated model and data loader each time, use reset() before get_score()\"\"\"\n",
        "        if ignore:\n",
        "            return 0\n",
        "        if model and loader:\n",
        "            self.reset(model, loader)\n",
        "        assert not self.need_reset, 'need reset model and loader before calculating NLL'\n",
        "        self.need_reset = True\n",
        "        return self.cal_nll()\n",
        "\n",
        "    def reset(self, model, loader):\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.need_reset = False\n",
        "\n",
        "    def cal_nll(self,model):\n",
        "        total_loss = 0\n",
        "        criterion = nn.NLLLoss()\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.loader):\n",
        "                inp, target = data\n",
        "                if self.gpu:\n",
        "                    inp, target = inp.cuda(), target.cuda()\n",
        "                h,c = self.model.init_state(inp.size()[0])\n",
        "                pred,_, _, _ = model.forward(inp, (h.to(device),c.to(device)))\n",
        "                for k in range(inp.size(1)):\n",
        "                    loss = criterion(pred[:,k,:], target[:,k].view(-1))\n",
        "                    total_loss += loss.item()\n",
        "                # print(torch.argmax(pred,dim=2).size(),target.view(-1).size())\n",
        "                \n",
        "        return total_loss / len(self.loader)"
      ],
      "metadata": {
        "id": "aLfEYdrtpIJa"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from posix import X_OK\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.hidden_dim =200\n",
        "        self.embedding_dim = 300\n",
        "        self.num_layers = 3\n",
        "        self.temperature = 0.1\n",
        "        n_vocab = args.vocab_l\n",
        "        self.embeddings = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "           self.embedding_dim, self.hidden_dim, batch_first=True,\n",
        "           \n",
        "        )\n",
        "        self.fc = nn.Linear(self.hidden_dim, n_vocab)\n",
        "        # self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        for param in self.parameters():\n",
        "            if param.requires_grad and len(param.shape) > 0:\n",
        "                stddev = 1 / math.sqrt(param.shape[0])\n",
        "                if args.dis_init == 'uniform':\n",
        "                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n",
        "                elif args.dis_init == 'normal':\n",
        "                    torch.nn.init.normal_(param, std=stddev)\n",
        "    def forward(self, inp, hidden,need_hidden= False):\n",
        "        self.theta = []\n",
        "        if args.gpu:\n",
        "                inp = inp.cuda()\n",
        "        emb = self.embeddings(inp)  # batch_size * len * embedding_dim\n",
        "        if len(inp.size()) == 1:\n",
        "                   emb = emb.unsqueeze(1)  # batch_size * 1 * embedding_dim\n",
        "        out, hidden = self.lstm(emb, hidden)\n",
        "        out = self.fc(out.squeeze(1)) \n",
        "        out = F.softmax(out, dim=-1)\n",
        "        # out.retain_grad()\n",
        "        self.theta.append(out)\n",
        "        gumbel_t = self.add_gumbel(out)\n",
        "        next_token = torch.argmax(gumbel_t*self.temperature, dim=1)\n",
        "        pred = F.softmax(gumbel_t * self.temperature, dim=-1)  # batch_size * vocab_size\n",
        "        next_token_onehot = None\n",
        "        return pred, hidden, next_token, next_token_onehot\n",
        "       \n",
        "\n",
        "    def sample_generate(self,inp,hidden,one_hot=False):\n",
        "      inp =torch.randint(0,args.vocab_l,(inp.size()[0],)).long()\n",
        "      batch_size = inp.size()[0]   \n",
        "      samples = torch.zeros( inp.size()[0], args.window).long()\n",
        "      if one_hot:\n",
        "            all_preds = torch.zeros( inp.size()[0], args.window, args.vocab_l)\n",
        "            if args.gpu:\n",
        "                all_preds = all_preds.cuda()\n",
        "      for i in range(args.window):\n",
        "                pred, hidden, next_token, next_token_onehot = self.forward(inp,hidden) \n",
        "                if one_hot:\n",
        "                    all_preds[:, i] = pred\n",
        "                samples[0:batch_size, i] = next_token\n",
        "      if one_hot:\n",
        "            return all_preds  # batch_size * seq_len * vocab_size\n",
        "      return samples\n",
        "\n",
        "\n",
        "    def add_gumbel(self,theta, eps=1e-10, gpu=args.gpu):\n",
        "        u = torch.zeros(theta.size())\n",
        "        if gpu:\n",
        "            u = u.cuda()\n",
        "        u.uniform_(0, 1)\n",
        "        gumbel_t = torch.log(theta + eps) - torch.log(-torch.log(u + eps) + eps)\n",
        "        return gumbel_t \n",
        "\n",
        "\n",
        "    def init_state(self, batch_size):\n",
        "        h = torch.zeros(1, batch_size, self.hidden_dim)\n",
        "        c = torch.zeros(1, batch_size, self.hidden_dim)\n",
        "        return h, c\n",
        "     \n",
        "dis_filter_sizes = [10, 15, 20, 25, 30, 4, 5, 2]\n",
        "dis_num_filters = [50, 50, 80, 80, 80, 50, 50, 50]\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "   def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # self.lstm_size = 128\n",
        "        self.embedding_dim = 300\n",
        "        self.num_layers = 3\n",
        "        self.feature_dim = sum(dis_num_filters)\n",
        "        # self.gpu = gpu\n",
        "        n_vocab = args.vocab_l\n",
        "        # self.embeddings = nn.Embedding(\n",
        "        #     num_embeddings=n_vocab,\n",
        "        #     embedding_dim=self.embedding_dim,\n",
        "        # )\n",
        "        self.embeddings = nn.Linear(args.vocab_l, self.embedding_dim, bias=False)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, n, (f, self.embedding_dim)) for (n, f) in zip(dis_num_filters , dis_filter_sizes)\n",
        "        ])\n",
        "        self.highway = nn.Linear(self.feature_dim, self.feature_dim)\n",
        "        self.feature2out = nn.Linear(self.feature_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.init_params()\n",
        "\n",
        "   def forward(self, x):\n",
        "        feature = self.get_feature(x)\n",
        "        out = self.feature2out(self.dropout(feature))\n",
        "        pred = nn.Sigmoid()(out.reshape(out.size()[0]))\n",
        "        return pred\n",
        "\n",
        "   def get_feature(self, inp):\n",
        "        emb = self.embeddings(inp).unsqueeze(1) # batch_size * 1 * max_seq_len * embed_dim\n",
        "        # print(emb.size(),\"*********\")\n",
        "        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs]  # [batch_size * num_filter * length]\n",
        "        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs]  # [batch_size * num_filter]\n",
        "        pred = torch.cat(pools, 1)  # tensor: batch_size * feature_dim\n",
        "        highway = self.highway(pred)\n",
        "        pred = torch.sigmoid(highway) * F.relu(highway) + (1. - torch.sigmoid(highway)) * pred  # highway\n",
        "        return pred\n",
        "\n",
        "   def init_params(self):\n",
        "        for param in self.parameters():\n",
        "            if param.requires_grad and len(param.shape) > 0:\n",
        "                stddev = 1 / math.sqrt(param.shape[0])\n",
        "                if args.dis_init == 'uniform':\n",
        "                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n",
        "                elif args.dis_init == 'normal':\n",
        "                    torch.nn.init.normal_(param, std=stddev)"
      ],
      "metadata": {
        "id": "OS1TMDigtTae"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from abc import abstractmethod\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import optim"
      ],
      "metadata": {
        "id": "9swOJhUp6Jn5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "class SGD(Optimizer):\n",
        "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False, *, maximize=False, foreach: Optional[bool] = None):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov,\n",
        "                        maximize=maximize, foreach=foreach)\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super().__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "            group.setdefault('maximize', False)\n",
        "            group.setdefault('foreach', None)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad = []\n",
        "            d_p_list = []\n",
        "            momentum_buffer_list = []\n",
        "            has_sparse_grad = False\n",
        "            i = 0 \n",
        "            for p in group['params']:\n",
        "               \n",
        "                if p.grad is not None:\n",
        "                    i += 1\n",
        "                    params_with_grad.append(p)\n",
        "                    d_p_list.append(p.grad)\n",
        "                    if p.grad.is_sparse:\n",
        "                        has_sparse_grad = True\n",
        "\n",
        "                    state = self.state[p]\n",
        "                    if 'momentum_buffer' not in state:\n",
        "                        momentum_buffer_list.append(None)\n",
        "                    else:\n",
        "                        momentum_buffer_list.append(state['momentum_buffer'])\n",
        "\n",
        "            sgd(params_with_grad,\n",
        "                d_p_list,\n",
        "                momentum_buffer_list,\n",
        "                weight_decay=group['weight_decay'],\n",
        "                momentum=group['momentum'],\n",
        "                lr=group['lr'],\n",
        "                dampening=group['dampening'],\n",
        "                nesterov=group['nesterov'],\n",
        "                maximize=group['maximize'],\n",
        "                has_sparse_grad=has_sparse_grad,\n",
        "                foreach=group['foreach'])\n",
        "\n",
        "            # update momentum_buffers in state\n",
        "            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
        "                state = self.state[p]\n",
        "                state['momentum_buffer'] = momentum_buffer\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "def sgd(params: List[Tensor],\n",
        "        d_p_list: List[Tensor],\n",
        "        momentum_buffer_list: List[Optional[Tensor]],\n",
        "        # kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\n",
        "        # setting this as kwarg for now as functional API is compiled by torch/distributed/optim\n",
        "        has_sparse_grad: bool = None,\n",
        "        foreach: bool = None,\n",
        "        *,\n",
        "        weight_decay: float,\n",
        "        momentum: float,\n",
        "        lr: float,\n",
        "        dampening: float,\n",
        "        nesterov: bool,\n",
        "        maximize: bool):\n",
        "    r\"\"\"Functional API that performs SGD algorithm computation.\n",
        "\n",
        "    See :class:`~torch.optim.SGD` for details.\n",
        "    \"\"\"\n",
        "\n",
        "    if foreach is None:\n",
        "        # Placeholder for more complex foreach logic to be added when value is not set\n",
        "        foreach = False\n",
        "\n",
        "    if foreach and torch.jit.is_scripting():\n",
        "        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n",
        "\n",
        "    if foreach and not torch.jit.is_scripting():\n",
        "        func = _multi_tensor_sgd\n",
        "    else:\n",
        "        func = _single_tensor_sgd\n",
        "    # print( func )\n",
        "    func(params,\n",
        "         d_p_list,\n",
        "         momentum_buffer_list,\n",
        "         weight_decay=weight_decay,\n",
        "         momentum=momentum,\n",
        "         lr=lr,\n",
        "         dampening=dampening,\n",
        "         nesterov=nesterov,\n",
        "         has_sparse_grad=has_sparse_grad,\n",
        "         maximize=maximize)\n",
        "\n",
        "def _single_tensor_sgd(params: List[Tensor],\n",
        "                       d_p_list: List[Tensor],\n",
        "                       momentum_buffer_list: List[Optional[Tensor]],\n",
        "                       *,\n",
        "                       weight_decay: float,\n",
        "                       momentum: float,\n",
        "                       lr: float,\n",
        "                       dampening: float,\n",
        "                       nesterov: bool,\n",
        "                       maximize: bool,\n",
        "                       has_sparse_grad: bool):\n",
        "\n",
        "    for i, param in enumerate(params):\n",
        "\n",
        "        d_p = d_p_list[i]\n",
        "        if weight_decay != 0:\n",
        "            d_p = d_p.add(param, alpha=weight_decay)\n",
        "\n",
        "        if momentum != 0:\n",
        "            buf = momentum_buffer_list[i]\n",
        "\n",
        "            if buf is None:\n",
        "                buf = torch.clone(d_p).detach()\n",
        "                momentum_buffer_list[i] = buf\n",
        "            else:\n",
        "                buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
        "\n",
        "            if nesterov:\n",
        "                d_p = d_p.add(buf, alpha=momentum)\n",
        "            else:\n",
        "                d_p = buf\n",
        "\n",
        "        alpha = lr if maximize else -lr\n",
        "        # pre_p = param.clone()\n",
        "        param.add_(d_p, alpha=alpha)\n",
        "        # print(pre_p - param)\n",
        "\n",
        "\n",
        "def _multi_tensor_sgd(params: List[Tensor],\n",
        "                      grads: List[Tensor],\n",
        "                      momentum_buffer_list: List[Optional[Tensor]],\n",
        "                      *,\n",
        "                      weight_decay: float,\n",
        "                      momentum: float,\n",
        "                      lr: float,\n",
        "                      dampening: float,\n",
        "                      nesterov: bool,\n",
        "                      maximize: bool,\n",
        "                      has_sparse_grad: bool):\n",
        "\n",
        "    if len(params) == 0:\n",
        "        return\n",
        "\n",
        "    if has_sparse_grad is None:\n",
        "        has_sparse_grad = any([grad.is_sparse for grad in grads])\n",
        "\n",
        "    if weight_decay != 0:\n",
        "        grads = torch._foreach_add(grads, params, alpha=weight_decay)\n",
        "\n",
        "    if momentum != 0:\n",
        "        bufs = []\n",
        "\n",
        "        all_states_with_momentum_buffer = True\n",
        "        for i in range(len(momentum_buffer_list)):\n",
        "            if momentum_buffer_list[i] is None:\n",
        "                all_states_with_momentum_buffer = False\n",
        "                break\n",
        "            else:\n",
        "                bufs.append(momentum_buffer_list[i])\n",
        "\n",
        "        if all_states_with_momentum_buffer:\n",
        "            torch._foreach_mul_(bufs, momentum)\n",
        "            torch._foreach_add_(bufs, grads, alpha=1 - dampening)\n",
        "        else:\n",
        "            bufs = []\n",
        "            for i in range(len(momentum_buffer_list)):\n",
        "                if momentum_buffer_list[i] is None:\n",
        "                    buf = momentum_buffer_list[i] = torch.clone(grads[i]).detach()\n",
        "                else:\n",
        "                    buf = momentum_buffer_list[i]\n",
        "                    buf.mul_(momentum).add_(grads[i], alpha=1 - dampening)\n",
        "\n",
        "                bufs.append(buf)\n",
        "\n",
        "        if nesterov:\n",
        "            torch._foreach_add_(grads, bufs, alpha=momentum)\n",
        "        else:\n",
        "            grads = bufs\n",
        "\n",
        "    alpha = lr if maximize else -lr\n",
        "    if not has_sparse_grad:\n",
        "        torch._foreach_add_(params, grads, alpha=alpha)\n",
        "    else:\n",
        "        # foreach APIs dont support sparse\n",
        "        for i in range(len(params)):\n",
        "            params[i].add_(grads[i], alpha=alpha)"
      ],
      "metadata": {
        "id": "e6R7tcrJjkD5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netD = Discriminator()\n",
        "netG = Generator()\n",
        "optimizerD = SGD(netD.parameters(), lr = 0.0001)\n",
        "optimizerG =SGD(netG.parameters(), lr = 0.0001)\n",
        "# print(netG.parameters())\n",
        "criterion = nn.BCELoss()\n",
        "nllloss = NLL(\"NLL\", netG, dataloader_dis, gpu=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "netD.to(device)\n",
        "netG.to(device) \n",
        "gloss = []\n",
        "dloss = []\n",
        "Nloss = []\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  print(epoch)\n",
        "  i = 0\n",
        "  for data_gen,data_dis in zip(dataloader_gen,dataloader_dis):\n",
        "    optimizerD.zero_grad()\n",
        "    optimizerG.zero_grad()\n",
        "    # for p1,p2 in zip(pre_param, netG.parameters()):\n",
        "    #   print(p1.grad == 0)\n",
        "    real, _ = data_dis\n",
        "    d_gen, _ = data_gen\n",
        "    input_g = Variable(d_gen).to(device)\n",
        "    input = Variable(real).to(device)\n",
        "    target = Variable(torch.ones(input.size()[0])).to(device)\n",
        "    state_h, state_c = netG.init_state(input_g.size()[0])\n",
        "    gen_samples = netG.sample_generate(input_g, (state_h.to(device), state_c.to(device)), one_hot=True)\n",
        "    if args.gpu:\n",
        "       real, gen_samples = real.cuda(), gen_samples.cuda()\n",
        "    real_samples = F.one_hot(real, args.vocab_l).float()\n",
        "    # real error for discriminator\n",
        "    output =  torch.reshape(netD(real_samples),(-1,)).to(device)\n",
        "    errD_real = criterion(output, target)\n",
        "    ###################################################################\n",
        "    output = netD(gen_samples)\n",
        "    # print(output)\n",
        "    target = Variable(torch.zeros(input_g.size()[0])).to(device)\n",
        "    errD_fake = criterion(torch.reshape(output, (-1,)), target)\n",
        "    errD = errD_real + errD_fake\n",
        "    errD.backward(retain_graph=True)\n",
        "    optimizerD.step()\n",
        "    # optimize Generator\n",
        "    target = Variable(torch.ones(input.size()[0])).to(device)\n",
        "    output = netD(gen_samples)\n",
        "    # print(output)\n",
        "    errG = criterion(torch.reshape(output, (-1,)), target)\n",
        "    # print(errG)\n",
        "    errG.backward()\n",
        "    pre_param = []\n",
        "   \n",
        "    if netG is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(netG.parameters(), 5)\n",
        "    optimizerG.step()\n",
        "    if i % 100== 0:\n",
        "      nlls = nllloss.cal_nll(netG)\n",
        "      print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f NLL_Loss:%4f' % (epoch, epochs, i, len(dataloader_gen), errD.detach(), errG.detach(),nlls))\n",
        "      gloss.append(errG.cpu().data.numpy())\n",
        "      dloss.append(errD.cpu().data.numpy())\n",
        "      Nloss.append(nlls)\n",
        "      #vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True)\n",
        "      state_h, state_c = netG.init_state(input_g.size()[0])\n",
        "      fake = netG.sample_generate(input_g.float(),(state_h.to(device), state_c.to(device)))\n",
        "      # print(fake[0,:])\n",
        "      str1 = \"\"\n",
        "      for o in fake:\n",
        "        str1 = \"\"\n",
        "        for w in o:  \n",
        "          # print(w)     \n",
        "          str1 += d_load.vocab[w]\n",
        "          str1 +=\" \"\n",
        "        print(str1)\n",
        "        break\n",
        "    i += 1\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(gloss)\n",
        "plt.plot(dloss)\n",
        "plt.plot(Nloss)\n",
        "plt.show()\n",
        "      #vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True)"
      ],
      "metadata": {
        "id": "aqDb0L95JC9k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56ad0f3b-5a92-45d5-9e6e-422258762791"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "0\n",
            "[0/10][0/313] Loss_D: 1.3638 Loss_G: 0.6652 NLL_Loss:-0.016518\n",
            "گرما حرص بی‌منتها تشنه درج برفتی بستان برین بسی چین باش سیف دریای سه گفتی برگ‌تر حرص حرکت نبیندش گذشت پاکیزه‌ست شیفته‌ی بازیچه‌ی کریجی یقینت صورت خیالت پا آنات بپوشیش \n",
            "[0/10][100/313] Loss_D: 1.3665 Loss_G: 0.6650 NLL_Loss:-0.016522\n",
            "شمع نه‌گر سپارم نیارد حباب معمور همخانه تمامی مژگان یقینت زمستان کافرینت برو سایه فریبنده سوادش لطیف نور‌بودی آخر خاص بی‌نیازی تباهی خاست لفظ شربت پراکنده شمع ثریاست الصلا تنزیل \n",
            "[0/10][200/313] Loss_D: 1.3647 Loss_G: 0.6647 NLL_Loss:-0.016511\n",
            "خدم بباید وز فتحه بی‌خردان غمدان کردار داغ شانه نیابند گسار جسمها پرس سکون شسته دوست خانه‌دار کژبین‌شده‌با رسوا عود غمدان سبزی ازو رستنی وا کردند رسید قمار وزان خواهم \n",
            "[0/10][300/313] Loss_D: 1.3634 Loss_G: 0.6643 NLL_Loss:-0.016503\n",
            "زشوق مژگان آفرید محنت پوش آر وانمودند همینها‌بود گفتم محفل تعمیر سکه زهد جبین فروزی لقا کشت تابد چترش برنا اولا جنس ناپیدا کرد بی‌منتها اظهار صدر نجوید فرود کس \n",
            "1\n",
            "[1/10][0/313] Loss_D: 1.3575 Loss_G: 0.6646 NLL_Loss:-0.016515\n",
            "مردان الله بقل دانند جوزا سجاده محابا خواب قرین گرد ببین خوبی سرا لله کافکند شریف فهم خانه خاری خندید چشمش بازی مگیر بی‌گنه نهاده بدمهر دیدار نهاد فرمان گوهران \n",
            "[1/10][100/313] Loss_D: 1.3574 Loss_G: 0.6642 NLL_Loss:-0.016517\n",
            "بوی eos پذیرد چند معدل دیبا ناپسندم پیلان در‌بند کشته پنجه تخصیص ناگفته یاری جوید چندن آن معما مصطفا بست‌گردن نوش بلایی دفتر هرگز پا رای حکم کچک زبورم عظم \n",
            "[1/10][200/313] Loss_D: 1.3573 Loss_G: 0.6644 NLL_Loss:-0.016522\n",
            "طپید سرادقات زنده ثنا شسته سجاده پروردگار صما ویرانی شیفته‌ی حوران اولاد سیمین بی‌بدل پادشایی دودی رقیبت کی صناعت وز درد بی‌پروا صحبت نخل دانش سخن مارا میان نهایاتش خشم \n",
            "[1/10][300/313] Loss_D: 1.3541 Loss_G: 0.6636 NLL_Loss:-0.016512\n",
            "نزمکان کجا نعمت عدیل آنکس چیزی معادا خسته حشم هست بزن احوال قوتها پاکیزه عاشقانت جوار بیشه سنگسار دیبا گذر دارا دارد اثر پر ویرانی یکسر نمای لذت نماندی ساربانا \n",
            "2\n",
            "[2/10][0/313] Loss_D: 1.3507 Loss_G: 0.6639 NLL_Loss:-0.016518\n",
            "دارد اینک مهیا خجالت بدانگونه صبری خانه‌آرایی رادی خدم چونین آراسته نخورده‌ست مستمندان افسانه‌ی مومن تسکین و برفتی پیرانه گر خلخ لولوست باد لذت درازا فلک مشفق سرم نامی جانور \n",
            "[2/10][100/313] Loss_D: 1.3499 Loss_G: 0.6633 NLL_Loss:-0.016512\n",
            "همسایه جسمها گذاشت بر دادند پاکیزه تری جمالش جنبشهای محابا حیوان پاکیزه‌ست چشمه چونان‌بود دیو خاطر رب وامق رنگ‌کردیم پنجه پایه کاغذ نمانده حلوا دیبای گنجی نمی‌آید جا شادیش ارکان \n",
            "[2/10][200/313] Loss_D: 1.3527 Loss_G: 0.6634 NLL_Loss:-0.016514\n",
            "ازیرا نفس‌های مفرح بنگر سوکواران فعل بدرود بی‌منتها توانگر مستمندان شهنشاهی مهره‌ی بی‌اختر پاکیزه کژی نان مطلب وان ناز همیشه شود تدبیر کشی پیرانه افسردگی تقویم اعتدال اندرین آورد کچک \n",
            "[2/10][300/313] Loss_D: 1.3496 Loss_G: 0.6633 NLL_Loss:-0.016514\n",
            "گوهرها اندوهگین مدحش گردانند گذاشت ملجا نیابد سر پسندد پسندد هویدا کرده طرفه کوی سایه همه نجوید نتوان بازیچه‌ی عاشقان آراید آموز همیشه وزو دو مده زهد جنگ قبل گفته‌ی \n",
            "3\n",
            "[3/10][0/313] Loss_D: 1.3440 Loss_G: 0.6634 NLL_Loss:-0.016525\n",
            "هفته سیمین مشک چونان‌بود تارک ازیراک تبه فردا فعل قبه‌ی جوحری پیش بن عهد نیش رو تلختر صورت اضداد بی‌اختر علم اثر میران افکنی پاکیزه سعی بابل بسمل رسید توانگر \n",
            "[3/10][100/313] Loss_D: 1.3444 Loss_G: 0.6635 NLL_Loss:-0.016515\n",
            "بزن رو شهر مسجد‌شده‌چون غم خاطر شبها یکره حیرت‌طرازیست گلشن چنو‌بودی چنگال علت تیر جا مادر خجالت ظاهر خوب بیدل گور اندوه منجا لیک حرص فراق لولو وش سگ نتوانیم \n",
            "[3/10][200/313] Loss_D: 1.3430 Loss_G: 0.6626 NLL_Loss:-0.016515\n",
            "ساحت برخاسته نور‌بودی صفات خانه مجروح قهار ذاتش صفاتش منشین دهر هامواره بی‌خردان برین جابلسا بازی حال بشتابی آرد طبع زبانم زبورم شکل نشود گرجیب ندانست سکر دوده خانه‌آرایی تمکین \n",
            "[3/10][300/313] Loss_D: 1.3419 Loss_G: 0.6632 NLL_Loss:-0.016520\n",
            "چکنم همی‌بکشد نیابند زیبا پیروزه PAD محنت بی‌نیاز sep نقش‌بندی شک همانا حرز مفتاح چه کرده‌ام دیبا ورا صحرانشین‌اند ذوقی للوئ هوای وهم فیضت مردمی‌بود اولا رویش مخرام سبزی فعل \n",
            "4\n",
            "[4/10][0/313] Loss_D: 1.3365 Loss_G: 0.6633 NLL_Loss:-0.016519\n",
            "خیالت روشن آدم تعریف میوزد شو بشتابی شناخت مفرد BOS یافتند بشکن فهم ارکانش کشته بستان مجروح دانستم آنات نیرنگ‌سازی پرور رای دیده نگار یری مواسا سایه وی تعمیر خواب \n",
            "[4/10][100/313] Loss_D: 1.3354 Loss_G: 0.6622 NLL_Loss:-0.016507\n",
            "دلت به مانده‌ام گفت کردگارت آتشی حشم چنگال همان سودای آیینه‌واریم قاصرات مانا تنزیل قندیل سعی تپد فیاض اشباع زهره‌ی افسردگی صائب سراسر کرده گشتیم بتیغ دیبای ویرانی سفر بینم \n",
            "[4/10][200/313] Loss_D: 1.3350 Loss_G: 0.6622 NLL_Loss:-0.016508\n",
            "مرده صما اقبال گره‌بست گذاریم قدح هل مستعجلم هجر نرود کوی یکسر بد نهاده گرفته ناپسندم نسیمی گیهان طواف فصلهای ستی الجلال اندوهگین شبها بیدل چین داند سموم فرقان شریف \n",
            "[4/10][300/313] Loss_D: 1.3347 Loss_G: 0.6626 NLL_Loss:-0.016520\n",
            "گنجش بگفتم نامست غواصست روی مکانی خشم صبری ساز گرچه پروردگار اندر کاوداشت مایه زبانی نرمی بگسست سایه هامواره مگشا ماجرا الجلال نبودی مدح وهم چندان‌که مده یاس باشم بکی \n",
            "5\n",
            "[5/10][0/313] Loss_D: 1.3266 Loss_G: 0.6622 NLL_Loss:-0.016520\n",
            "وجه ناخن بین چندی خوب رویان کلاه از کافکند دانه رهنمایان درونم محو حرکت پاک شانه وصف گذر اشارت فرو اصطناع دغل صورتی رها تعمیر مگس نبیندش اول چنان بسان \n",
            "[5/10][100/313] Loss_D: 1.3283 Loss_G: 0.6624 NLL_Loss:-0.016514\n",
            "ذاتش صائب مدام زنی اشارت تو اشک گو لب است مباش همینها‌بود دلم لاغست‌ای مگیر باز گیتی راست جان مدغم مفرد چونان دیبا ثناگفتن ساقیان فزود سالها افشاندش بن تمامی \n",
            "[5/10][200/313] Loss_D: 1.3289 Loss_G: 0.6621 NLL_Loss:-0.016518\n",
            "ربودی برومند دارا تمامی دستی‌که شخص بنمای بیابد جانور بیرون به مرحمت مجنون گوی یاس دیو همی چندان‌که زبانم نمی‌آید بگسست ممکن او‌بودیم تنها باری توایم‌ای شیخ یکتا همتش صحبت \n",
            "[5/10][300/313] Loss_D: 1.3268 Loss_G: 0.6620 NLL_Loss:-0.016518\n",
            "شوی قصر اجرام بدسگالان لیک پرور ظالم توحید پیکان حق مانا متعاقب‌بود نفس‌های فعل ناز دان فصلهای نگهبان جنگ سربلندی زنهار آراید چنانچون ولیکن گفتم برگ‌تر درویش درمانش می‌روشن کسم \n",
            "6\n",
            "[6/10][0/313] Loss_D: 1.3219 Loss_G: 0.6622 NLL_Loss:-0.016515\n",
            "باطل خورشید بچگان یری وانمودند عبرت زحمت‌بود دشمن ذاکر نتوان فرتوتی ظالم هوای رویان پیرانه چنگال تشبیه الوان خندید منور درهای سوخته مهتر مستعجلم حوا جوحری نخواهد بنیاد چشم تعالی \n",
            "[6/10][100/313] Loss_D: 1.3228 Loss_G: 0.6619 NLL_Loss:-0.016511\n",
            "زهد چنگال مجلس ثناگفتن جبین مکافات گرم فتوح کافر فخر بنگر نشود دشمن وهم طاقت تمجید آنات رفتار شوی زنده جودش قبل رسته سوخته دوست‌بود فکرت آراید تنگ پاشی پیلگون \n",
            "[6/10][200/313] Loss_D: 1.3215 Loss_G: 0.6620 NLL_Loss:-0.016518\n",
            "ظالم برجست مباش شو » زهد ور برون اولا میازار ذرهم فرمان بخشش دید ماند کون نگردی‌گر کریجی بی‌قیمتند ره زدیم قیمت کافکند آیینه‌ی آنگاه سان زحمت‌بود سینه‌ها تری نیارد \n",
            "[6/10][300/313] Loss_D: 1.3197 Loss_G: 0.6612 NLL_Loss:-0.016513\n",
            "که خواب مکن نامه افکنی خدم زشوق عز بیرون دامن بادام درو جاری مطلب مکن گرجیب نکنی دلبر مساوا مشکل دان گفت گناهی شبها درازا بداند گرما بدانگونه هویت گردان \n",
            "7\n",
            "[7/10][0/313] Loss_D: 1.3159 Loss_G: 0.6616 NLL_Loss:-0.016512\n",
            "شب اظهار نوری طفلی شگرف تقویم پهنا بنمای شاید هزار کجایی کژی بس هرچند عمل کام پدیدار حیوان شهدا آورده‌ست کجاهستش عاری می‌را شریف دلم زه‌ای کاوداشت قدح کرده‌ستی قدرتش \n",
            "[7/10][100/313] Loss_D: 1.3152 Loss_G: 0.6614 NLL_Loss:-0.016511\n",
            "خشم حمد نفس‌های ابری ذوقی قدسا نخورد آبگون تشنه سرو ثم روشن زار میری حیف خضرا کردگارت اهل درآمد آدم مشرق الوان رقیبت همه نشود صنع گونه نیکوان بیشی نور‌بودی \n",
            "[7/10][200/313] Loss_D: 1.3139 Loss_G: 0.6615 NLL_Loss:-0.016516\n",
            "کای دوست مادر صفاتش هم بگشای درونم سپاهش حی صهبا پیداکجاست جامه صائب باری دید هوا مدهوش مغبر جسمها شیدا فساد گرانمایه مقادیرست بی‌اختر دلبر پالایدش رعیت سلب بشتابد مقدار \n",
            "[7/10][300/313] Loss_D: 1.3140 Loss_G: 0.6617 NLL_Loss:-0.016519\n",
            "بلندی باقی گو خواهد ندانی یکی باقی فیاض نداند دعوی سپاس تپد نقش‌بندی تحقیق قوت میری قبه‌ی مخلصان وانمودند و‌بود گوید خیالت زمانی قوتها ماورای بتواند بیابان ناگهان کیست تمامست \n",
            "8\n",
            "[8/10][0/313] Loss_D: 1.3091 Loss_G: 0.6615 NLL_Loss:-0.016512\n",
            "نابینا نور‌بودی لختکیمیل باشد افتادیم تختش ولی گردیهای حیف نمانده داغ میعاد فزود مانا جمالش تشدیدشان بهره عروسان امیرا چو ثنای سوهان‌زده یاری بی‌گنه سوادش منتها آینه‌ی مرا خورد همانا \n",
            "[8/10][100/313] Loss_D: 1.3076 Loss_G: 0.6612 NLL_Loss:-0.016521\n",
            "ها بکی گوش سوی چندان حد عاقلی قول شادکامی مدهوش ترسان خوبی لیلی کان کافرم آب بپیوسته‌ست می جود روح آورد جابلسا آرد خویی مسجد‌شده‌چون بیدار بپیر مباش امروز بیاموز \n",
            "[8/10][200/313] Loss_D: 1.3070 Loss_G: 0.6618 NLL_Loss:-0.016518\n",
            "دلبر ارکانش گردون لذت نه‌ای گفتم و درونم قهار ناله‌های این حال بی‌نیازی راستی آسمان کلاه تنها نشود‌ای پیشینگان PAD صحرا بینم دی ماه هر ساعت جهان وجه خمار کین‌شده \n",
            "[8/10][300/313] Loss_D: 1.3057 Loss_G: 0.6612 NLL_Loss:-0.016521\n",
            "عادل شبها ساحل سراید مسجد اعضا فتح بگردانید رادی آوازه میفروز تمامی شبها گشتیم زهد بوده‌ست طاقت شادی یقین جوهری بی‌روزن باش گوشه‌ی رنگ کردار غایبند درازا یدالله بادام صاحب \n",
            "9\n",
            "[9/10][0/313] Loss_D: 1.3029 Loss_G: 0.6614 NLL_Loss:-0.016516\n",
            "خورد نزمکان گردباد دانست آسمان یغما کان گدا همواره یکسر مرغزار قلم افسردگی ناخن منور انده دهر گوش سنجاب طواف محیط شان خشک بیدلان آدمی این‌بند جابلسا صانع عرض لیلی \n",
            "[9/10][100/313] Loss_D: 1.2995 Loss_G: 0.6614 NLL_Loss:-0.016518\n",
            "خانمانها هست قدرت دینار دریا رویش دستمزد تن بداند باد دیده‌ی بلایی ملکوت میری خوانند برابر حل برابر همینها‌بود حرف کشی وا رسته تعمیر ماند چنو‌بودی دارا باشم کمال تافت \n",
            "[9/10][200/313] Loss_D: 1.3000 Loss_G: 0.6617 NLL_Loss:-0.016510\n",
            "پادشا خیره صد چندی بوده‌ست چینی و بی‌بها مساوا حیوان سنگ مرده درازا می‌کند لفظی برومند همی‌رفت تباهی فال نیز فلک پسندد وانمودند مست من بزن برنهی تابستان دیدن زیباست \n",
            "[9/10][300/313] Loss_D: 1.2970 Loss_G: 0.6616 NLL_Loss:-0.016510\n",
            "گه کسم مولا طواف ثم راد ثنا صهبا نهایاتش سعی نخستین درمانش مدام ده غرض خرما نوازی املا ناایمن معانی خانه ستاره گفتم گره‌بست آب چندین نامه روم حالیا‌گر دانست \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.randint(3, 5, (1,))"
      ],
      "metadata": {
        "id": "MQN0Grfz3tvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4614b2-4d5d-4328-d402-1b497f8310e3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.grad"
      ],
      "metadata": {
        "id": "x2XvYLyh4uo1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "632f5094-ca92-4477-bff4-a4200d03668b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-d40920619642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poems = pd.read_csv(\"/content/drive/MyDrive/NLU_Project/Data/test.csv\")\n",
        "# print(poems['poem_id'])\n",
        "i = 3\n",
        "poems_sequence =[]\n",
        "while True:\n",
        "            poem_i = poems[poems['poem_id'] == i]\n",
        "            # print(poem_i)\n",
        "            index_i = poems.index[poems['poem_id'] == i]\n",
        "            current_poem = \"\"\n",
        "            for p in index_i:\n",
        "                v_position = poem_i.loc[p,\"v_position\"]\n",
        "                verse =  poem_i.loc[p,\"poem_text\"]\n",
        "                # print(verse)\n",
        "                current_poem += verse \n",
        "                if v_position == 0:\n",
        "                    current_poem += \" \\t\"\n",
        "                if v_position == 1:\n",
        "                    current_poem += \" \\n\"\n",
        "            if len(current_poem)>0:\n",
        "              poems_sequence.append(current_poem)\n",
        "            i += 1\n",
        "            if i>596 :\n",
        "              break\n"
      ],
      "metadata": {
        "id": "NK5ZuS8HVPmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_h, state_c = netG.init_state(1)\n",
        "fake= netG(torch.tensor([[1]]), (state_h, state_c))"
      ],
      "metadata": {
        "id": "0f_nxAh2UOpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
        "for o in fake:\n",
        "        str1 = \"\"\n",
        "        for w in o:  \n",
        "          # print(w)     \n",
        "          str1 += d_load.vocab[w]\n",
        "          str1 +=\" \"\n",
        "          score = 0\n",
        "          poem = ''\n",
        "        print(str1)\n",
        "        for poems in poems_sequence:\n",
        "          s =  sentence_bleu(poems.split(\" \"),str1.split(\" \"))\n",
        "          if score < s:\n",
        "           score = s\n",
        "           poem = poems\n",
        "        print(score)\n",
        "        print(poem)\n"
      ],
      "metadata": {
        "id": "BC6DZZCJUuuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZiVq8Ywiy-bY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}