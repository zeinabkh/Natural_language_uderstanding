# -*- coding: utf-8 -*-
"""NLU1_8_19.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IfmZnDSZozD3ZHw5befvO1Zlff6AKxht
"""

!pip install hazm

!pip install gensim

# import fasttext
# import fasttext.util
from gensim.models import Word2Vec
import numpy as np

# Commented out IPython magic to ensure Python compatibility.
try:
#   %tensorflow 2.0x
except:
  pass

import os
import numpy as np 
import hazm
import re
from collections import Counter

stemmer = hazm.Stemmer()

from google.colab import drive
drive.mount('/content/drive')

os.chdir("/content/drive/MyDrive/NLU/HW1")

train_sentences = open("train_data.txt",'r').read()

label_to_id = {
    'O' : 0,
    "e":1,
    "ye":2,
    "ve" :3 ,
    '@e'  :4,
    'y':5,
  
}

id_to_label = {
    0:'O',
    1:'e',
    2:'ye',
    3:"ve" ,
    4:"@e",
    5:'y',
  
}

# Commented out IPython magic to ensure Python compatibility.
# %%time
# train_sentences = open("train_data.txt",'r').read()
# words_labels_list = train_sentences.split("\n")
# del train_sentences
# tokens = []
# for line in words_labels_list:
#     clean_word = line.split("\t")[0].strip().replace('ÙŠ', 'ÛŒ').replace('Ùƒ', 'Ú©').replace('Ø©', 'Ù‡').replace('\u200c', "")
#     tokens.append(clean_word)
# count_tokens = Counter(np.array(tokens))
# word_list  = sorted(list(count_tokens.keys()))
# keys_index = {w: i+2 for i, w in enumerate(word_list) }
# keys_index["pad"] = 0
# keys_index["unseen"] = 1
# del count_tokens
# del words_labels_list

len(keys_index)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# data_path = '/content/drive/MyDrive/NLU/HW1/train_data.txt'
# x_train, sent = [], []
# y_train, ezafe_tags = [], []
# with open(data_path) as f:
#     for line in f:
#       w = line.strip().split('\t')
#       if w[0] != '.':
#         if len(line.strip().split('\t')) == 2:
#           word, ezafe_tag = line.strip().split('\t')
#           e_word = re.search(r"[A-Z a-z]", word)
#           digit = re.search(r"[0-9]", word)
#           if not (e_word or digit):
#             sent.append(keys_index[word.replace('ÙŠ', 'ÛŒ').replace('Ùƒ', 'Ú©').replace('Ø©', 'Ù‡').replace('\u200c', "")])
#             ezafe_tags.append(label_to_id[ezafe_tag])
#       else:
#         x_train.append(sent)
#         y_train.append(ezafe_tags)                  
#         sent = []
#         ezafe_tags = []

from keras.preprocessing.sequence import pad_sequences
x_train = pad_sequences(x_train, maxlen=30, padding='post')
y_train = pad_sequences(y_train, maxlen=30, padding='post')

from sklearn.metrics import accuracy_score
from keras.models import Sequential
from keras.layers import Dense,Input
from keras.layers import LSTM, Embedding, Bidirectional, TimeDistributed,Dense
from keras.layers import Dropout
import tensorflow as tf
import numpy as np
from sklearn.metrics import accuracy_score

tf.keras.backend.clear_session()
predict_model = Sequential()
predict_model.add(Embedding(len(word_list)+2,700 ))
predict_model.add(Bidirectional(LSTM(units = 50,input_shape= (30,700), return_sequences=True)))
predict_model.add(TimeDistributed(Dense(6,activation='softmax')))

tf.keras.utils.plot_model(predict_model,expand_nested=True,show_shapes=True)

len(word_list)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# tf.keras.backend.clear_session()
# predict_model.compile(optimizer="adam",loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])
# predict_model.fit(x_train, y_train,epochs=4,validation_split=0.1,batch_size= 256)

predict_model.summary()

"""# **test**"""

from sklearn.metrics import accuracy_score, precision_recall_curve, precision_score,recall_score,confusion_matrix
from matplotlib import  pyplot as plt
import pandas as pd
import seaborn as sn

# Commented out IPython magic to ensure Python compatibility.
# %%time
# data_path = '/content/drive/MyDrive/NLU/HW1/test_data.txt'
# x_test, sent = [], []
# y_test, ezafe_tags = [], []
# with open(data_path) as f:
#     for line in f:
#       w = line.strip().split('\t')
#       if w[0] != '.':
#         if len(line.strip().split('\t')) == 2:
#           word, ezafe_tag = line.strip().split('\t')
#           e_word = re.search(r"[A-Z a-z]", word)
#           digit = re.search(r"[0-9]", word)
#           if not (e_word or digit):
#             try:
#                sent.append(keys_index[word.replace('ÙŠ', 'ÛŒ').replace('Ùƒ', 'Ú©').replace('Ø©', 'Ù‡').replace('\u200c', "")])
#                ezafe_tags.append(label_to_id[ezafe_tag])
#             except KeyError:
#                sent.append(keys_index["unseen"])
#                ezafe_tags.append(label_to_id[ezafe_tag])
#       else:
#           try:
#                sent.append(keys_index[word.replace('ÙŠ', 'ÛŒ').replace('Ùƒ', 'Ú©').replace('Ø©', 'Ù‡').replace('\u200c', "")])
#                ezafe_tags.append(label_to_id[ezafe_tag])
#           except KeyError:
#                sent.append(keys_index["unseen"])
#                ezafe_tags.append(label_to_id[ezafe_tag])
#           x_test.append(sent)
#           y_test.append(ezafe_tags)                  
#           sent = []
#           ezafe_tags = []

for y in y_test:
  if len(y)==0:
     print(len(y))

def plt_confusion_matrix(con_mat,p,t):
    ax = plt.axes()
    print(con_mat.shape)
    df_cm = pd.DataFrame(con_mat, [id_to_label[i] for i in t],
                         [id_to_label[i] for i in t])
    print(df_cm)
    sn.set(font_scale=1)
    sn.heatmap(df_cm, cmap="RdYlGn", annot=True, annot_kws={"size": 10}, ax=ax, fmt='g')
    ax.set_xlabel('predict')
    ax.set_ylabel('true')
    ax.set_title('Normalized Confusion Matrix (%)')
    plt.show()
def evaluate(x_test,y_test):
  y_true = np.array([])
  y_predict = []

  for i,sample in enumerate(x_test[:100000]):
    print(i)
    try:
        sent_result  = predict_model.predict(np.array([sample]))
        y_true = np.concatenate((y_true, y_test[i]))
        for j ,word_result in enumerate(sent_result[0,:,:]):
              y_predict.append(word_result.argmax())
    except ValueError:
      print(sample)
      continue

  print("accuracy: ",accuracy_score(y_true,y_predict))
  print("precision micro: ",precision_score(y_true,y_predict,average="micro"))
  print("precision macro: ",precision_score(y_true,y_predict,average="macro"))
  print("recall micro: ",recall_score(y_true,y_predict,average="micro"))
  print("recall macro: ",recall_score(y_true,y_predict,average="macro"))
  print(np.unique(y_predict),np.unique(y_true))
  plt_confusion_matrix(confusion_matrix(y_true,y_predict),np.unique(y_predict),np.unique(y_true))

evaluate(x_test,y_test)

np.argmax(np.array([1,3,4,2,7,5,91]))



w2v_model = Word2Vec.load("/content/drive/MyDrive/IR_data/w2v_150k_hazm.model")
# del w2v_model

# predict_model.predict(x_train[3,:,:].reshape(1,50,100)).shape
s = ""
x = []
for i in s.split(" "):
  x.append(w2v_model.wv[i])

p_l = predict_model.predict(np.array(x).reshape(1,len(x),100))
# print(p_l)
for p in p_l[0]:
     print(id_to_label[p.argmax()])
  #  print(id_to_label() )

"""# **BERT**

# load and tokenize data
"""

from transformers import PreTrainedTokenizer, BertForTokenClassification, BertTokenizer
from transformers import  TFTrainer, TFTrainingArguments
import tensorflow as tf

#  looad data 
def data_loader(path):
    train_sentences = open(path,'r').read()
    words_labels_list = train_sentences.split("\n")
    del train_sentences
    sentences = []
    labels_sequence = []
    normalizer = hazm.Normalizer()
    x = ""
    y = []
    max_len = 50 
    for l in words_labels_list[:4000000]:
       w = l.split("\t")
       if len(w)<2:
         continue
       if w[0].strip() != ".":
          x += " "+ w[0].strip()
          y.append(label_to_id[w[1].strip()])
       else:
        x+=  " "+ w[0].strip()
        y.append(label_to_id[w[1].strip()])
        # print(x)
        sentences.append(normalizer.normalize(x))
        labels_sequence.append(y)
        del x
        del y 
        x = " "
        y = []
    return sentences,labels_sequence

# Commented out IPython magic to ensure Python compatibility.
# %%time
# train_texts, label_train =data_loader("train_data.txt")

tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)
tokenizer.pad_token = "[PAD]"
train_token_id = tokenizer(train_texts, truncation=True, padding= True)

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_token_id),
    label_train
))

"""# Fine_tune"""

training_args = TFTrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)

with training_args.strategy.scope():
    model = BertForTokenClassification.from_pretrained("bert-base-multilingual-cased")

trainer = TFTrainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    # eval_dataset=val_dataset             # evaluation dataset
)

trainer.train()