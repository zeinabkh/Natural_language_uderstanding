# -*- coding: utf-8 -*-
"""NLU1_8_21_bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SPtsgBKC0wVyA7UUIqz5yily-CD1SBsl
"""

label_to_id = {
    'O' : 0,
    "e":1,
    "ye":2,
    "ve" :3 ,
    '@e'  :4,
    'y':5,
    0:6
  
}

id_to_label = {
    0:'O',
    1:'e',
    2:'ye',
    3:"ve" ,
    4:"@e",
    5:'y',
    6:0
  
}

"""# **BERT**

# load and tokenize data

# Fine_tune
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers
!pip install datasets
!pip install seqeval

import os
import itertools
import pandas as pd
import numpy as np
import re
from datasets import Dataset
from datasets import load_metric
from transformers import AutoTokenizer
from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer
from transformers import DataCollatorForTokenClassification
import torch

# label_list = ['O','B-MISC','I-MISC','B-PER','I-PER','B-ORG','I-ORG','B-LOC','I-LOC']
# label_encoding_dict = {'I-PRG': 2,'I-I-MISC': 2, 'I-OR': 6, 'O': 0, 'I-': 0, 'VMISC': 0, 'B-PER': 3, 'I-PER': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-LOC': 7, 'I-LOC': 8, 'B-MISC': 1, 'I-MISC': 2}

task = "ner" 
model_checkpoint = "HooshvareLab/bert-fa-base-uncased"
batch_size = 16
    
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def get_words_and_ezafe_tags(filename):
  
    x_train, sent = [], []
    y_train, ezafe_tags = [], []
    with open(filename) as f:
        i = 0
        for line in f:
           
            w = line.strip().split('\t')
            if w[0] != '.':
                if len(line.strip().split('\t')) == 2:
                     word, ezafe_tag = line.strip().split('\t')
                e_word = re.search(r"[A-Z a-z]", word)
                digit = re.search(r"[0-9]", word)
                if not (e_word or digit):
                    sent.append(word.replace('ي', 'ی').replace('ك', 'ک').replace('ة', 'ه').replace('\u200c', ""))
                    ezafe_tags.append(ezafe_tag)
            else:
                 if len(line.strip().split('\t')) == 2:
                     word, ezafe_tag = line.strip().split('\t')
                 sent.append(word.replace('ي', 'ی').replace('ك', 'ک').replace('ة', 'ه').replace('\u200c', ""))
                 ezafe_tags.append(ezafe_tag)
                 x_train.append(sent)
                 y_train.append(ezafe_tags)                  
                 sent = []
                 ezafe_tags = []
    return pd.DataFrame({'words': x_train, 'ezafe_tag': y_train})
  
def get_un_token_dataset(train_path, test_path):
    train_df =get_words_and_ezafe_tags(train_path)
    print(len(train_df))
    # test_df = get_words_and_ezafe_tags(test_path)
    train_dataset = Dataset.from_pandas(train_df[:200000])
    # test_dataset = Dataset.from_pandas(test_df)
    test_dataset = Dataset.from_pandas(train_df[200000:])
    # print(type(train_df['ezafe_tag']))
    return (train_dataset, test_dataset)

train_dataset, test_dataset = get_un_token_dataset('/content/drive/MyDrive/NLU/HW1/train_data.txt', '/content/drive/MyDrive/NLU/HW1/test_data.txt')

def tokenize_and_align_labels(examples):

    label_all_tokens = True
    tokenized_inputs = tokenizer(list(examples["words"]), truncation=True, is_split_into_words=True,padding=True,max_length=50)
    labels = []
    # print(examples['ezafe_tag'])
    for i, label in enumerate(examples['ezafe_tag']):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:

            if word_idx is None:
                label_ids.append(-100)
            elif label[word_idx] == '0':
                label_ids.append(6)
            else:
                label_ids.append(label_to_id[label[word_idx]])
            # else:
                # label_ids.append(label_to_id[label[word_idx]] if label_all_tokens else -100)
            # previous_word_idx = word_idx
        # if len(label_ids) != len(word_ids):
          # print("badbakhti")
        # print(len(label_ids),len(word_ids))
        labels.append(label_ids)
        
    tokenized_inputs["labels"] = labels
    return tokenized_inputs


train_tokenized_datasets = train_dataset.map(tokenize_and_align_labels, batched=True)
test_tokenized_datasets = test_dataset.map(tokenize_and_align_labels, batched=True)

test_dataset

del train_dataset
del test_dataset
batch_size =64

import torch
torch.cuda.empty_cache()
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_to_id.keys()))
torch.cuda.empty_cache()
args = TrainingArguments(
    evaluation_strategy = "epoch",
    learning_rate=1e-4,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=4,
    weight_decay=1e-5,
    output_dir = "/output"

)

data_collator = DataCollatorForTokenClassification(tokenizer)
metric = load_metric("seqeval")
torch.cuda.empty_cache()

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_predictions = [[id_to_label[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]
    true_labels = [[id_to_label[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]
    print(true_predictions)
    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {"precision": results["overall_precision"], "recall": results["overall_recall"], "f1": results["overall_f1"], "accuracy": results["overall_accuracy"]}
    
trainer = Trainer(
    model,
    args,
    train_dataset=train_tokenized_datasets,
    eval_dataset=test_tokenized_datasets,
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()
trainer.evaluate()
trainer.save_model('un-ezafe.model')

tokenizer = AutoTokenizer.from_pretrained('./output/un-ezafe.model')
model = AutoModelForTokenClassification.from_pretrained('./output/un-ezafe.model', num_labels=7)
sent = ""
ezafe_tags = []
y_true = np.array([])
y_pred =  np.array([])
with open('/content/drive/MyDrive/NLU/HW1/test_data.txt') as f:
        i = 0
        for line in f:
            w = line.strip().split('\t')
            if w[0] != '.':
                if len(line.strip().split('\t')) == 2:
                     word, ezafe_tag = line.strip().split('\t')
                e_word = re.search(r"[A-Z a-z]", word)
                digit = re.search(r"[0-9]", word)
                if not (e_word or digit):
                    sent += word.replace('ي', 'ی').replace('ك', 'ک').replace('ة', 'ه').replace('\u200c', "")
                    ezafe_tags.append(label_to_id(ezafe_tag))
            else:
                 if len(line.strip().split('\t')) == 2:
                     word, ezafe_tag = line.strip().split('\t')
                 sent += word.replace('ي', 'ی').replace('ك', 'ک').replace('ة', 'ه').replace('\u200c', "")
                 ezafe_tags.append(label_to_id(ezafe_tag))
                 tokens = tokenizer(sent)
                 sent = []
                 ezafe_tags = []
                 torch.tensor(tokens['input_ids']).unsqueeze(0).size()
                 predictions = model.forward(input_ids=torch.tensor(tokens['input_ids']).unsqueeze(0), attention_mask=torch.tensor(tokens['attention_mask']).unsqueeze(0))
                 
                 predictions = torch.argmax(predictions.logits.squeeze(), axis=1)
                 y_pred = np.concatenate((y_pred,np.array(predictions)))
                 y_true = np.concatenate((y_true,nparray(ezafe_tags)))
                #  predictions = [label_[i] for i in preds]
                #  words = tokenizer.batch_decode(tokens['input_ids'])
                #  pd.DataFrame({'ezafe': predictions, 'words': words}).to_csv('un_ner.csv')
print(len(y_pred),len(y_true))